2015-10-17 17:53:31,956 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-17 17:53:33,061 WARN org.apache.hadoop.metrics2.impl.MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-datanode.properties,hadoop-metrics2.properties
2015-10-17 17:53:33,145 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-17 17:53:33,145 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-17 17:53:33,146 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is localhost.localdomain
2015-10-17 17:53:33,147 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-17 17:53:33,191 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-17 17:53:33,205 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-17 17:53:33,325 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-17 17:53:33,331 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-17 17:53:33,352 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-17 17:53:33,359 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-17 17:53:33,359 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-17 17:53:33,359 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-17 17:53:33,377 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-10-17 17:53:33,381 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-10-17 17:53:33,381 INFO org.mortbay.log: jetty-6.1.26
2015-10-17 17:53:33,792 INFO org.mortbay.log: Started SelectChannelConnector@0.0.0.0:50075
2015-10-17 17:53:34,114 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-17 17:53:34,137 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-17 17:53:34,176 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-17 17:53:34,195 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-17 17:53:34,225 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-17 17:53:34,233 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to CentOS.Hadoop/127.0.0.1:8020 starting to offer service
2015-10-17 17:53:34,242 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-17 17:53:34,243 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-17 17:53:34,719 INFO org.apache.hadoop.hdfs.server.common.Storage: Data-node version: -55 and name-node layout version: -55
2015-10-17 17:53:34,746 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/in_use.lock acquired by nodename 3464@CentOS.Hadoop
2015-10-17 17:53:34,747 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /var/lib/hadoop-hdfs/cache/hdfs/dfs/data is not formatted
2015-10-17 17:53:34,747 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2015-10-17 17:53:34,895 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-340015217-127.0.0.1-1445079105718
2015-10-17 17:53:34,895 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled
2015-10-17 17:53:34,895 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current/BP-340015217-127.0.0.1-1445079105718 is not formatted.
2015-10-17 17:53:34,895 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2015-10-17 17:53:34,895 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting block pool BP-340015217-127.0.0.1-1445079105718 directory /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current/BP-340015217-127.0.0.1-1445079105718/current
2015-10-17 17:53:34,904 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-10-17 17:53:34,905 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=113353421;bpid=BP-340015217-127.0.0.1-1445079105718;lv=-55;nsInfo=lv=-55;cid=CID-1ea80fc5-7f68-47f8-8615-fee51275fc9c;nsid=113353421;c=0;bpid=BP-340015217-127.0.0.1-1445079105718;dnuuid=null
2015-10-17 17:53:34,906 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Generated and persisted new Datanode UUID e3272006-5ef0-4545-8fb1-4cf4435a0d4d
2015-10-17 17:53:34,945 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current, StorageType: DISK
2015-10-17 17:53:34,975 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-10-17 17:53:34,986 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1445093549986 with interval 21600000
2015-10-17 17:53:34,994 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-340015217-127.0.0.1-1445079105718
2015-10-17 17:53:34,994 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-340015217-127.0.0.1-1445079105718 on volume /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current...
2015-10-17 17:53:35,042 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-340015217-127.0.0.1-1445079105718 on /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current: 48ms
2015-10-17 17:53:35,042 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-340015217-127.0.0.1-1445079105718: 48ms
2015-10-17 17:53:35,045 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-340015217-127.0.0.1-1445079105718 on volume /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current...
2015-10-17 17:53:35,045 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-340015217-127.0.0.1-1445079105718 on volume /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current: 0ms
2015-10-17 17:53:35,045 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 3ms
2015-10-17 17:53:35,047 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid null) service to CentOS.Hadoop/127.0.0.1:8020 beginning handshake with NN
2015-10-17 17:53:35,154 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid null) service to CentOS.Hadoop/127.0.0.1:8020 successfully registered with NN
2015-10-17 17:53:35,154 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode CentOS.Hadoop/127.0.0.1:8020 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-10-17 17:53:35,376 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid e3272006-5ef0-4545-8fb1-4cf4435a0d4d) service to CentOS.Hadoop/127.0.0.1:8020 trying to claim ACTIVE state with txid=1
2015-10-17 17:53:35,376 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid e3272006-5ef0-4545-8fb1-4cf4435a0d4d) service to CentOS.Hadoop/127.0.0.1:8020
2015-10-17 17:53:35,432 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 0 blocks total. Took 1 msec to generate and 55 msecs for RPC and NN processing.  Got back commands none
2015-10-17 17:53:35,436 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2015-10-17 17:53:35,436 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-10-17 17:53:35,437 INFO org.apache.hadoop.util.GSet: 0.5% max memory 966.7 MB = 4.8 MB
2015-10-17 17:53:35,437 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2015-10-17 17:53:35,438 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-340015217-127.0.0.1-1445079105718
2015-10-17 17:53:35,444 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-340015217-127.0.0.1-1445079105718 to blockPoolScannerMap, new size=1
2015-10-17 17:56:52,657 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-17 17:56:52,660 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
2015-10-17 18:00:47,758 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-17 18:00:49,865 WARN org.apache.hadoop.metrics2.impl.MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-datanode.properties,hadoop-metrics2.properties
2015-10-17 18:00:50,031 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-17 18:00:50,031 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-17 18:00:50,037 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is localhost.localdomain
2015-10-17 18:00:50,038 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-17 18:00:50,148 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-17 18:00:50,161 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-17 18:00:50,423 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-17 18:00:50,426 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-17 18:00:50,440 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-17 18:00:50,442 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-17 18:00:50,442 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-17 18:00:50,442 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-17 18:00:50,490 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-10-17 18:00:50,492 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-10-17 18:00:50,492 INFO org.mortbay.log: jetty-6.1.26
2015-10-17 18:00:51,027 INFO org.mortbay.log: Started SelectChannelConnector@0.0.0.0:50075
2015-10-17 18:00:51,534 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-17 18:00:51,550 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-17 18:00:51,632 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-17 18:00:51,658 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-17 18:00:51,693 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-17 18:00:51,723 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to CentOS.Hadoop/127.0.0.1:8020 starting to offer service
2015-10-17 18:00:51,725 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-17 18:00:51,727 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-17 18:00:53,000 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: CentOS.Hadoop/127.0.0.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-17 18:00:54,000 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: CentOS.Hadoop/127.0.0.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-17 18:00:55,001 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: CentOS.Hadoop/127.0.0.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-17 18:00:56,002 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: CentOS.Hadoop/127.0.0.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-17 18:00:57,003 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: CentOS.Hadoop/127.0.0.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-17 18:00:57,371 INFO org.apache.hadoop.hdfs.server.common.Storage: Data-node version: -55 and name-node layout version: -55
2015-10-17 18:00:57,396 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/in_use.lock acquired by nodename 2099@CentOS.Hadoop
2015-10-17 18:00:57,583 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-340015217-127.0.0.1-1445079105718
2015-10-17 18:00:57,583 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled
2015-10-17 18:00:57,584 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-10-17 18:00:57,586 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=113353421;bpid=BP-340015217-127.0.0.1-1445079105718;lv=-55;nsInfo=lv=-55;cid=CID-1ea80fc5-7f68-47f8-8615-fee51275fc9c;nsid=113353421;c=0;bpid=BP-340015217-127.0.0.1-1445079105718;dnuuid=e3272006-5ef0-4545-8fb1-4cf4435a0d4d
2015-10-17 18:00:57,620 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current, StorageType: DISK
2015-10-17 18:00:57,778 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-10-17 18:00:57,785 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1445091497785 with interval 21600000
2015-10-17 18:00:57,791 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-340015217-127.0.0.1-1445079105718
2015-10-17 18:00:57,792 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-340015217-127.0.0.1-1445079105718 on volume /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current...
2015-10-17 18:00:57,821 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current/BP-340015217-127.0.0.1-1445079105718/current: 24576
2015-10-17 18:00:57,828 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-340015217-127.0.0.1-1445079105718 on /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current: 36ms
2015-10-17 18:00:57,828 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-340015217-127.0.0.1-1445079105718: 37ms
2015-10-17 18:00:57,828 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-340015217-127.0.0.1-1445079105718 on volume /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current...
2015-10-17 18:00:57,829 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-340015217-127.0.0.1-1445079105718 on volume /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current: 0ms
2015-10-17 18:00:57,829 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 1ms
2015-10-17 18:00:57,832 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid null) service to CentOS.Hadoop/127.0.0.1:8020 beginning handshake with NN
2015-10-17 18:00:57,950 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid null) service to CentOS.Hadoop/127.0.0.1:8020 successfully registered with NN
2015-10-17 18:00:57,950 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode CentOS.Hadoop/127.0.0.1:8020 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-10-17 18:00:58,189 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid e3272006-5ef0-4545-8fb1-4cf4435a0d4d) service to CentOS.Hadoop/127.0.0.1:8020 trying to claim ACTIVE state with txid=25
2015-10-17 18:00:58,189 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid e3272006-5ef0-4545-8fb1-4cf4435a0d4d) service to CentOS.Hadoop/127.0.0.1:8020
2015-10-17 18:00:58,301 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 0 blocks total. Took 0 msec to generate and 111 msecs for RPC and NN processing.  Got back commands none
2015-10-17 18:00:58,314 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2015-10-17 18:00:58,314 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-10-17 18:00:58,315 INFO org.apache.hadoop.util.GSet: 0.5% max memory 966.7 MB = 4.8 MB
2015-10-17 18:00:58,315 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2015-10-17 18:00:58,320 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-340015217-127.0.0.1-1445079105718
2015-10-17 18:00:58,326 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-340015217-127.0.0.1-1445079105718 to blockPoolScannerMap, new size=1
2015-10-17 18:17:29,968 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 7948ms
2015-10-17 18:42:51,234 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-17 18:42:51,282 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
2015-10-17 18:42:57,041 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-17 18:42:58,023 WARN org.apache.hadoop.metrics2.impl.MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-datanode.properties,hadoop-metrics2.properties
2015-10-17 18:42:58,147 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-17 18:42:58,147 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-17 18:42:58,149 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: File descriptor passing is enabled.
2015-10-17 18:42:58,161 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is localhost.localdomain
2015-10-17 18:42:58,162 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-17 18:42:58,369 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-17 18:42:58,377 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-17 18:42:58,381 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Exception in secureMain
2015-10-17 18:42:58,389 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1
2015-10-17 18:42:58,391 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
2015-10-17 18:49:10,538 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-17 18:49:11,503 WARN org.apache.hadoop.metrics2.impl.MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-datanode.properties,hadoop-metrics2.properties
2015-10-17 18:49:11,682 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-17 18:49:11,682 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-17 18:49:11,684 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: File descriptor passing is enabled.
2015-10-17 18:49:11,689 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is localhost.localdomain
2015-10-17 18:49:11,690 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-17 18:49:11,778 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-17 18:49:11,780 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-17 18:49:11,786 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-17 18:49:11,786 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Listening on UNIX domain socket: /var/run/hadoop-hdfs/dn.50010
2015-10-17 18:49:12,019 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-17 18:49:12,026 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-17 18:49:12,045 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-17 18:49:12,049 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-17 18:49:12,049 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-17 18:49:12,050 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-17 18:49:12,065 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-10-17 18:49:12,067 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-10-17 18:49:12,067 INFO org.mortbay.log: jetty-6.1.26
2015-10-17 18:49:12,575 INFO org.mortbay.log: Started SelectChannelConnector@0.0.0.0:50075
2015-10-17 18:49:13,269 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-17 18:49:13,307 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-17 18:49:13,392 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-17 18:49:13,422 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-17 18:49:13,436 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-17 18:49:13,445 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to CentOS.Hadoop/127.0.0.1:8020 starting to offer service
2015-10-17 18:49:13,453 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-17 18:49:13,454 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-17 18:49:13,690 INFO org.apache.hadoop.hdfs.server.common.Storage: Data-node version: -55 and name-node layout version: -55
2015-10-17 18:49:13,712 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/in_use.lock acquired by nodename 6340@CentOS.Hadoop
2015-10-17 18:49:13,937 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-340015217-127.0.0.1-1445079105718
2015-10-17 18:49:13,937 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled
2015-10-17 18:49:13,938 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-10-17 18:49:13,939 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=113353421;bpid=BP-340015217-127.0.0.1-1445079105718;lv=-55;nsInfo=lv=-55;cid=CID-1ea80fc5-7f68-47f8-8615-fee51275fc9c;nsid=113353421;c=0;bpid=BP-340015217-127.0.0.1-1445079105718;dnuuid=e3272006-5ef0-4545-8fb1-4cf4435a0d4d
2015-10-17 18:49:13,964 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current, StorageType: DISK
2015-10-17 18:49:14,140 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-10-17 18:49:14,163 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1445100616163 with interval 21600000
2015-10-17 18:49:14,169 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-340015217-127.0.0.1-1445079105718
2015-10-17 18:49:14,183 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-340015217-127.0.0.1-1445079105718 on volume /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current...
2015-10-17 18:49:14,216 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current/BP-340015217-127.0.0.1-1445079105718/current: 28672
2015-10-17 18:49:14,223 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-340015217-127.0.0.1-1445079105718 on /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current: 39ms
2015-10-17 18:49:14,223 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-340015217-127.0.0.1-1445079105718: 54ms
2015-10-17 18:49:14,223 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-340015217-127.0.0.1-1445079105718 on volume /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current...
2015-10-17 18:49:14,224 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-340015217-127.0.0.1-1445079105718 on volume /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current: 1ms
2015-10-17 18:49:14,226 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 3ms
2015-10-17 18:49:14,228 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid null) service to CentOS.Hadoop/127.0.0.1:8020 beginning handshake with NN
2015-10-17 18:49:14,263 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid null) service to CentOS.Hadoop/127.0.0.1:8020 successfully registered with NN
2015-10-17 18:49:14,263 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode CentOS.Hadoop/127.0.0.1:8020 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-10-17 18:49:14,406 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid e3272006-5ef0-4545-8fb1-4cf4435a0d4d) service to CentOS.Hadoop/127.0.0.1:8020 trying to claim ACTIVE state with txid=32
2015-10-17 18:49:14,407 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid e3272006-5ef0-4545-8fb1-4cf4435a0d4d) service to CentOS.Hadoop/127.0.0.1:8020
2015-10-17 18:49:14,477 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 0 blocks total. Took 1 msec to generate and 69 msecs for RPC and NN processing.  Got back commands none
2015-10-17 18:49:14,494 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2015-10-17 18:49:14,494 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-10-17 18:49:14,504 INFO org.apache.hadoop.util.GSet: 0.5% max memory 966.7 MB = 4.8 MB
2015-10-17 18:49:14,504 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2015-10-17 18:49:14,504 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-340015217-127.0.0.1-1445079105718
2015-10-17 18:49:14,515 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-340015217-127.0.0.1-1445079105718 to blockPoolScannerMap, new size=1
2015-10-17 18:56:12,770 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-17 18:56:12,888 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
2015-10-17 18:56:20,526 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-17 18:56:21,452 WARN org.apache.hadoop.metrics2.impl.MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-datanode.properties,hadoop-metrics2.properties
2015-10-17 18:56:21,616 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-17 18:56:21,616 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-17 18:56:21,620 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: File descriptor passing is enabled.
2015-10-17 18:56:21,633 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is localhost.localdomain
2015-10-17 18:56:21,633 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-17 18:56:21,796 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-17 18:56:21,803 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-17 18:56:21,812 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-17 18:56:21,812 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Listening on UNIX domain socket: /var/run/hadoop-hdfs/dn.50010
2015-10-17 18:56:22,013 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-17 18:56:22,016 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-17 18:56:22,031 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-17 18:56:22,036 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-17 18:56:22,036 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-17 18:56:22,036 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-17 18:56:22,242 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-10-17 18:56:22,247 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-10-17 18:56:22,247 INFO org.mortbay.log: jetty-6.1.26
2015-10-17 18:56:22,731 INFO org.mortbay.log: Started SelectChannelConnector@0.0.0.0:50075
2015-10-17 18:56:23,304 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-17 18:56:23,331 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-17 18:56:23,440 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-17 18:56:23,479 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-17 18:56:23,500 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-17 18:56:23,532 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to CentOS.Hadoop/127.0.0.1:8020 starting to offer service
2015-10-17 18:56:23,552 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-17 18:56:23,553 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-17 18:56:23,839 INFO org.apache.hadoop.hdfs.server.common.Storage: Data-node version: -55 and name-node layout version: -55
2015-10-17 18:56:23,872 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/in_use.lock acquired by nodename 6952@CentOS.Hadoop
2015-10-17 18:56:24,043 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-340015217-127.0.0.1-1445079105718
2015-10-17 18:56:24,043 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled
2015-10-17 18:56:24,044 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-10-17 18:56:24,045 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=113353421;bpid=BP-340015217-127.0.0.1-1445079105718;lv=-55;nsInfo=lv=-55;cid=CID-1ea80fc5-7f68-47f8-8615-fee51275fc9c;nsid=113353421;c=0;bpid=BP-340015217-127.0.0.1-1445079105718;dnuuid=e3272006-5ef0-4545-8fb1-4cf4435a0d4d
2015-10-17 18:56:24,070 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current, StorageType: DISK
2015-10-17 18:56:24,211 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-10-17 18:56:24,224 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1445091324224 with interval 21600000
2015-10-17 18:56:24,226 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-340015217-127.0.0.1-1445079105718
2015-10-17 18:56:24,241 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-340015217-127.0.0.1-1445079105718 on volume /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current...
2015-10-17 18:56:24,268 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current/BP-340015217-127.0.0.1-1445079105718/current: 28672
2015-10-17 18:56:24,271 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-340015217-127.0.0.1-1445079105718 on /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current: 29ms
2015-10-17 18:56:24,271 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-340015217-127.0.0.1-1445079105718: 45ms
2015-10-17 18:56:24,275 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-340015217-127.0.0.1-1445079105718 on volume /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current...
2015-10-17 18:56:24,275 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-340015217-127.0.0.1-1445079105718 on volume /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current: 0ms
2015-10-17 18:56:24,275 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 4ms
2015-10-17 18:56:24,277 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid null) service to CentOS.Hadoop/127.0.0.1:8020 beginning handshake with NN
2015-10-17 18:56:24,297 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid null) service to CentOS.Hadoop/127.0.0.1:8020 successfully registered with NN
2015-10-17 18:56:24,298 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode CentOS.Hadoop/127.0.0.1:8020 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-10-17 18:56:24,401 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid e3272006-5ef0-4545-8fb1-4cf4435a0d4d) service to CentOS.Hadoop/127.0.0.1:8020 trying to claim ACTIVE state with txid=32
2015-10-17 18:56:24,401 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid e3272006-5ef0-4545-8fb1-4cf4435a0d4d) service to CentOS.Hadoop/127.0.0.1:8020
2015-10-17 18:56:24,458 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 0 blocks total. Took 6 msec to generate and 51 msecs for RPC and NN processing.  Got back commands none
2015-10-17 18:56:24,467 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2015-10-17 18:56:24,467 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-10-17 18:56:24,478 INFO org.apache.hadoop.util.GSet: 0.5% max memory 966.7 MB = 4.8 MB
2015-10-17 18:56:24,478 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2015-10-17 18:56:24,478 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-340015217-127.0.0.1-1445079105718
2015-10-17 18:56:24,484 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-340015217-127.0.0.1-1445079105718 to blockPoolScannerMap, new size=1
2015-10-17 18:56:36,316 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
2015-10-17 18:56:40,307 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: CentOS.Hadoop/127.0.0.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-17 18:56:41,309 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: CentOS.Hadoop/127.0.0.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-17 18:56:42,310 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: CentOS.Hadoop/127.0.0.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-17 18:56:43,311 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: CentOS.Hadoop/127.0.0.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-17 18:56:44,312 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: CentOS.Hadoop/127.0.0.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-17 18:56:44,566 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeCommand action : DNA_REGISTER from CentOS.Hadoop/127.0.0.1:8020 with active state
2015-10-17 18:56:44,582 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid e3272006-5ef0-4545-8fb1-4cf4435a0d4d) service to CentOS.Hadoop/127.0.0.1:8020 beginning handshake with NN
2015-10-17 18:56:44,597 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid e3272006-5ef0-4545-8fb1-4cf4435a0d4d) service to CentOS.Hadoop/127.0.0.1:8020 successfully registered with NN
2015-10-17 18:56:44,609 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 0 blocks total. Took 0 msec to generate and 12 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@7cedd9bc
2015-10-17 18:56:44,609 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-340015217-127.0.0.1-1445079105718
2015-10-17 18:58:26,958 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-17 18:58:26,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
2015-10-17 19:00:22,120 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-17 19:00:29,597 WARN org.apache.hadoop.metrics2.impl.MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-datanode.properties,hadoop-metrics2.properties
2015-10-17 19:00:29,650 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-17 19:00:29,650 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-17 19:00:29,674 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: File descriptor passing is enabled.
2015-10-17 19:00:29,804 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is localhost.localdomain
2015-10-17 19:00:29,806 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-17 19:00:30,652 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-17 19:00:30,745 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-17 19:00:30,754 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-17 19:00:30,754 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Listening on UNIX domain socket: /var/run/hadoop-hdfs/dn.50010
2015-10-17 19:00:31,023 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-17 19:00:31,030 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-17 19:00:31,039 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-17 19:00:31,041 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-17 19:00:31,041 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-17 19:00:31,041 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-17 19:00:31,160 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-10-17 19:00:31,169 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-10-17 19:00:31,170 INFO org.mortbay.log: jetty-6.1.26
2015-10-17 19:00:32,705 INFO org.mortbay.log: Started SelectChannelConnector@0.0.0.0:50075
2015-10-17 19:00:33,780 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-17 19:00:33,855 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-17 19:00:34,029 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-17 19:00:34,074 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-17 19:00:34,159 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-17 19:00:34,231 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to CentOS.Hadoop/127.0.0.1:8020 starting to offer service
2015-10-17 19:00:34,255 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-17 19:00:34,247 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-17 19:00:36,233 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: CentOS.Hadoop/127.0.0.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-17 19:00:37,234 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: CentOS.Hadoop/127.0.0.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-17 19:00:37,864 INFO org.apache.hadoop.hdfs.server.common.Storage: Data-node version: -55 and name-node layout version: -55
2015-10-17 19:00:37,892 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/in_use.lock acquired by nodename 2476@CentOS.Hadoop
2015-10-17 19:00:38,030 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-340015217-127.0.0.1-1445079105718
2015-10-17 19:00:38,030 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled
2015-10-17 19:00:38,039 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-10-17 19:00:38,040 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=113353421;bpid=BP-340015217-127.0.0.1-1445079105718;lv=-55;nsInfo=lv=-55;cid=CID-1ea80fc5-7f68-47f8-8615-fee51275fc9c;nsid=113353421;c=0;bpid=BP-340015217-127.0.0.1-1445079105718;dnuuid=e3272006-5ef0-4545-8fb1-4cf4435a0d4d
2015-10-17 19:00:38,075 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current, StorageType: DISK
2015-10-17 19:00:38,210 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-10-17 19:00:38,213 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1445093943213 with interval 21600000
2015-10-17 19:00:38,224 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-340015217-127.0.0.1-1445079105718
2015-10-17 19:00:38,228 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-340015217-127.0.0.1-1445079105718 on volume /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current...
2015-10-17 19:00:38,361 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current/BP-340015217-127.0.0.1-1445079105718/current: 28672
2015-10-17 19:00:38,370 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-340015217-127.0.0.1-1445079105718 on /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current: 142ms
2015-10-17 19:00:38,370 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-340015217-127.0.0.1-1445079105718: 146ms
2015-10-17 19:00:38,373 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-340015217-127.0.0.1-1445079105718 on volume /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current...
2015-10-17 19:00:38,396 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-340015217-127.0.0.1-1445079105718 on volume /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current: 23ms
2015-10-17 19:00:38,397 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 26ms
2015-10-17 19:00:38,399 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid null) service to CentOS.Hadoop/127.0.0.1:8020 beginning handshake with NN
2015-10-17 19:00:38,465 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid null) service to CentOS.Hadoop/127.0.0.1:8020 successfully registered with NN
2015-10-17 19:00:38,465 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode CentOS.Hadoop/127.0.0.1:8020 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-10-17 19:00:38,565 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid e3272006-5ef0-4545-8fb1-4cf4435a0d4d) service to CentOS.Hadoop/127.0.0.1:8020 trying to claim ACTIVE state with txid=34
2015-10-17 19:00:38,565 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid e3272006-5ef0-4545-8fb1-4cf4435a0d4d) service to CentOS.Hadoop/127.0.0.1:8020
2015-10-17 19:00:38,612 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 0 blocks total. Took 1 msec to generate and 46 msecs for RPC and NN processing.  Got back commands none
2015-10-17 19:00:38,616 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2015-10-17 19:00:38,616 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-10-17 19:00:38,617 INFO org.apache.hadoop.util.GSet: 0.5% max memory 966.7 MB = 4.8 MB
2015-10-17 19:00:38,617 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2015-10-17 19:00:38,617 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-340015217-127.0.0.1-1445079105718
2015-10-17 19:00:38,620 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-340015217-127.0.0.1-1445079105718 to blockPoolScannerMap, new size=1
2015-10-17 19:11:12,663 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-17 19:11:12,685 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
2015-10-17 22:58:16,007 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-17 22:58:17,724 WARN org.apache.hadoop.metrics2.impl.MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-datanode.properties,hadoop-metrics2.properties
2015-10-17 22:58:17,903 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-17 22:58:17,903 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-17 22:58:17,907 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: File descriptor passing is enabled.
2015-10-17 22:58:17,910 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is localhost.localdomain
2015-10-17 22:58:17,911 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-17 22:58:18,020 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-17 22:58:18,033 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-17 22:58:18,036 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-17 22:58:18,036 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Listening on UNIX domain socket: /var/run/hadoop-hdfs/dn.50010
2015-10-17 22:58:18,211 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-17 22:58:18,214 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-17 22:58:18,222 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-17 22:58:18,228 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-17 22:58:18,229 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-17 22:58:18,229 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-17 22:58:18,270 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-10-17 22:58:18,273 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-10-17 22:58:18,273 INFO org.mortbay.log: jetty-6.1.26
2015-10-17 22:58:18,892 INFO org.mortbay.log: Started SelectChannelConnector@0.0.0.0:50075
2015-10-17 22:58:19,612 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-17 22:58:19,641 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-17 22:58:19,762 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-17 22:58:19,805 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-17 22:58:19,845 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-17 22:58:19,881 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to CentOS.Hadoop/127.0.0.1:8020 starting to offer service
2015-10-17 22:58:19,888 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-17 22:58:19,891 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-17 22:58:21,132 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: CentOS.Hadoop/127.0.0.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-17 22:58:22,132 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: CentOS.Hadoop/127.0.0.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-17 22:58:23,133 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: CentOS.Hadoop/127.0.0.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-17 22:58:24,133 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: CentOS.Hadoop/127.0.0.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-17 22:58:24,500 INFO org.apache.hadoop.hdfs.server.common.Storage: Data-node version: -55 and name-node layout version: -55
2015-10-17 22:58:24,521 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/in_use.lock acquired by nodename 2678@CentOS.Hadoop
2015-10-17 22:58:24,735 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-340015217-127.0.0.1-1445079105718
2015-10-17 22:58:24,735 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled
2015-10-17 22:58:24,736 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-10-17 22:58:24,739 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=113353421;bpid=BP-340015217-127.0.0.1-1445079105718;lv=-55;nsInfo=lv=-55;cid=CID-1ea80fc5-7f68-47f8-8615-fee51275fc9c;nsid=113353421;c=0;bpid=BP-340015217-127.0.0.1-1445079105718;dnuuid=e3272006-5ef0-4545-8fb1-4cf4435a0d4d
2015-10-17 22:58:24,763 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current, StorageType: DISK
2015-10-17 22:58:24,897 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-10-17 22:58:24,906 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1445103589906 with interval 21600000
2015-10-17 22:58:24,911 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-340015217-127.0.0.1-1445079105718
2015-10-17 22:58:24,915 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-340015217-127.0.0.1-1445079105718 on volume /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current...
2015-10-17 22:58:24,974 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-340015217-127.0.0.1-1445079105718 on /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current: 59ms
2015-10-17 22:58:24,974 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-340015217-127.0.0.1-1445079105718: 63ms
2015-10-17 22:58:24,977 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-340015217-127.0.0.1-1445079105718 on volume /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current...
2015-10-17 22:58:24,978 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-340015217-127.0.0.1-1445079105718 on volume /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current: 1ms
2015-10-17 22:58:24,980 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 6ms
2015-10-17 22:58:24,982 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid null) service to CentOS.Hadoop/127.0.0.1:8020 beginning handshake with NN
2015-10-17 22:58:25,069 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid null) service to CentOS.Hadoop/127.0.0.1:8020 successfully registered with NN
2015-10-17 22:58:25,069 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode CentOS.Hadoop/127.0.0.1:8020 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-10-17 22:58:25,276 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid e3272006-5ef0-4545-8fb1-4cf4435a0d4d) service to CentOS.Hadoop/127.0.0.1:8020 trying to claim ACTIVE state with txid=40
2015-10-17 22:58:25,276 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid e3272006-5ef0-4545-8fb1-4cf4435a0d4d) service to CentOS.Hadoop/127.0.0.1:8020
2015-10-17 22:58:25,366 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 0 blocks total. Took 1 msec to generate and 86 msecs for RPC and NN processing.  Got back commands none
2015-10-17 22:58:25,374 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2015-10-17 22:58:25,374 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-10-17 22:58:25,376 INFO org.apache.hadoop.util.GSet: 0.5% max memory 966.7 MB = 4.8 MB
2015-10-17 22:58:25,376 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2015-10-17 22:58:25,377 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-340015217-127.0.0.1-1445079105718
2015-10-17 22:58:25,379 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-340015217-127.0.0.1-1445079105718 to blockPoolScannerMap, new size=1
2015-10-17 23:01:33,101 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1532ms
2015-10-17 23:05:35,608 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1849ms
2015-10-17 23:06:24,255 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1653ms
2015-10-17 23:07:12,482 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 3253ms
2015-10-17 23:49:52,622 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1253ms
2015-10-17 23:58:40,601 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 3373ms
2015-10-17 23:58:59,983 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2310ms
2015-10-18 00:07:34,150 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-18 00:07:34,213 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
2015-10-18 00:09:57,387 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
2015-10-18 00:09:57,465 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-18 00:09:59,244 WARN org.apache.hadoop.metrics2.impl.MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-datanode.properties,hadoop-metrics2.properties
2015-10-18 00:09:59,436 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-18 00:09:59,436 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-18 00:09:59,444 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: File descriptor passing is enabled.
2015-10-18 00:09:59,450 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is localhost.localdomain
2015-10-18 00:09:59,452 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-18 00:09:59,585 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-18 00:09:59,601 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-18 00:09:59,605 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-18 00:09:59,605 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Listening on UNIX domain socket: /var/run/hadoop-hdfs/dn.50010
2015-10-18 00:09:59,922 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-18 00:09:59,925 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-18 00:09:59,933 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-18 00:09:59,934 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-18 00:09:59,934 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-18 00:09:59,934 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-18 00:09:59,979 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-10-18 00:09:59,986 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-10-18 00:09:59,986 INFO org.mortbay.log: jetty-6.1.26
2015-10-18 00:10:00,631 INFO org.mortbay.log: Started SelectChannelConnector@0.0.0.0:50075
2015-10-18 00:10:01,280 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-18 00:10:01,331 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-18 00:10:01,427 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-18 00:10:01,475 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-18 00:10:01,514 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-18 00:10:01,551 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to CentOS.Hadoop/127.0.0.1:8020 starting to offer service
2015-10-18 00:10:01,553 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-18 00:10:01,553 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-18 00:10:02,830 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: CentOS.Hadoop/127.0.0.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 00:10:03,831 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: CentOS.Hadoop/127.0.0.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 00:10:04,833 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: CentOS.Hadoop/127.0.0.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 00:10:05,834 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: CentOS.Hadoop/127.0.0.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 00:10:06,835 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: CentOS.Hadoop/127.0.0.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 00:10:07,349 INFO org.apache.hadoop.hdfs.server.common.Storage: Data-node version: -55 and name-node layout version: -55
2015-10-18 00:10:07,374 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/in_use.lock acquired by nodename 2659@CentOS.Hadoop
2015-10-18 00:10:07,559 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-340015217-127.0.0.1-1445079105718
2015-10-18 00:10:07,559 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled
2015-10-18 00:10:07,560 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-10-18 00:10:07,562 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=113353421;bpid=BP-340015217-127.0.0.1-1445079105718;lv=-55;nsInfo=lv=-55;cid=CID-1ea80fc5-7f68-47f8-8615-fee51275fc9c;nsid=113353421;c=0;bpid=BP-340015217-127.0.0.1-1445079105718;dnuuid=e3272006-5ef0-4545-8fb1-4cf4435a0d4d
2015-10-18 00:10:07,593 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current, StorageType: DISK
2015-10-18 00:10:07,735 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-10-18 00:10:07,744 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1445114696744 with interval 21600000
2015-10-18 00:10:07,749 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-340015217-127.0.0.1-1445079105718
2015-10-18 00:10:07,752 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-340015217-127.0.0.1-1445079105718 on volume /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current...
2015-10-18 00:10:07,807 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current/BP-340015217-127.0.0.1-1445079105718/current: 28672
2015-10-18 00:10:07,812 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-340015217-127.0.0.1-1445079105718 on /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current: 60ms
2015-10-18 00:10:07,813 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-340015217-127.0.0.1-1445079105718: 64ms
2015-10-18 00:10:07,815 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-340015217-127.0.0.1-1445079105718 on volume /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current...
2015-10-18 00:10:07,815 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-340015217-127.0.0.1-1445079105718 on volume /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current: 0ms
2015-10-18 00:10:07,815 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 2ms
2015-10-18 00:10:07,817 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid null) service to CentOS.Hadoop/127.0.0.1:8020 beginning handshake with NN
2015-10-18 00:10:07,912 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid null) service to CentOS.Hadoop/127.0.0.1:8020 successfully registered with NN
2015-10-18 00:10:07,912 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode CentOS.Hadoop/127.0.0.1:8020 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-10-18 00:10:08,095 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid e3272006-5ef0-4545-8fb1-4cf4435a0d4d) service to CentOS.Hadoop/127.0.0.1:8020 trying to claim ACTIVE state with txid=53
2015-10-18 00:10:08,095 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid e3272006-5ef0-4545-8fb1-4cf4435a0d4d) service to CentOS.Hadoop/127.0.0.1:8020
2015-10-18 00:10:08,147 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 0 blocks total. Took 1 msec to generate and 51 msecs for RPC and NN processing.  Got back commands none
2015-10-18 00:10:08,150 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2015-10-18 00:10:08,151 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-10-18 00:10:08,151 INFO org.apache.hadoop.util.GSet: 0.5% max memory 966.7 MB = 4.8 MB
2015-10-18 00:10:08,151 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2015-10-18 00:10:08,152 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-340015217-127.0.0.1-1445079105718
2015-10-18 00:10:08,154 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-340015217-127.0.0.1-1445079105718 to blockPoolScannerMap, new size=1
2015-10-18 00:17:16,801 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-18 00:17:16,809 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
2015-10-18 01:02:10,479 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-18 01:02:12,393 WARN org.apache.hadoop.metrics2.impl.MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-datanode.properties,hadoop-metrics2.properties
2015-10-18 01:02:12,618 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-18 01:02:12,618 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-18 01:02:12,628 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: File descriptor passing is enabled.
2015-10-18 01:02:12,633 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is localhost.localdomain
2015-10-18 01:02:12,634 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-18 01:02:12,747 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-18 01:02:12,764 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-18 01:02:12,767 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-18 01:02:12,767 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Listening on UNIX domain socket: /var/run/hadoop-hdfs/dn.50010
2015-10-18 01:02:13,182 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-18 01:02:13,185 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-18 01:02:13,193 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-18 01:02:13,200 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-18 01:02:13,200 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-18 01:02:13,200 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-18 01:02:13,259 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-10-18 01:02:13,266 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-10-18 01:02:13,266 INFO org.mortbay.log: jetty-6.1.26
2015-10-18 01:02:13,934 INFO org.mortbay.log: Started SelectChannelConnector@0.0.0.0:50075
2015-10-18 01:02:21,720 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-18 01:02:21,800 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-18 01:02:22,171 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-18 01:02:22,235 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-18 01:02:22,265 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-18 01:02:22,318 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to CentOS.Hadoop/127.0.0.1:8020 starting to offer service
2015-10-18 01:02:22,351 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-18 01:02:22,348 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-18 01:02:23,717 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: CentOS.Hadoop/127.0.0.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 01:02:24,717 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: CentOS.Hadoop/127.0.0.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 01:02:25,415 INFO org.apache.hadoop.hdfs.server.common.Storage: Data-node version: -55 and name-node layout version: -55
2015-10-18 01:02:25,447 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/in_use.lock acquired by nodename 2672@CentOS.Hadoop
2015-10-18 01:02:25,802 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-340015217-127.0.0.1-1445079105718
2015-10-18 01:02:25,802 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled
2015-10-18 01:02:25,807 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-10-18 01:02:25,812 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=113353421;bpid=BP-340015217-127.0.0.1-1445079105718;lv=-55;nsInfo=lv=-55;cid=CID-1ea80fc5-7f68-47f8-8615-fee51275fc9c;nsid=113353421;c=0;bpid=BP-340015217-127.0.0.1-1445079105718;dnuuid=e3272006-5ef0-4545-8fb1-4cf4435a0d4d
2015-10-18 01:02:25,873 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current, StorageType: DISK
2015-10-18 01:02:26,142 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-10-18 01:02:26,157 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1445122633157 with interval 21600000
2015-10-18 01:02:26,167 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-340015217-127.0.0.1-1445079105718
2015-10-18 01:02:26,169 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-340015217-127.0.0.1-1445079105718 on volume /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current...
2015-10-18 01:02:26,252 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-340015217-127.0.0.1-1445079105718 on /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current: 83ms
2015-10-18 01:02:26,252 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-340015217-127.0.0.1-1445079105718: 85ms
2015-10-18 01:02:26,256 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-340015217-127.0.0.1-1445079105718 on volume /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current...
2015-10-18 01:02:26,256 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-340015217-127.0.0.1-1445079105718 on volume /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current: 0ms
2015-10-18 01:02:26,256 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 4ms
2015-10-18 01:02:26,258 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid null) service to CentOS.Hadoop/127.0.0.1:8020 beginning handshake with NN
2015-10-18 01:02:26,434 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid null) service to CentOS.Hadoop/127.0.0.1:8020 successfully registered with NN
2015-10-18 01:02:26,434 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode CentOS.Hadoop/127.0.0.1:8020 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-10-18 01:02:26,792 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid e3272006-5ef0-4545-8fb1-4cf4435a0d4d) service to CentOS.Hadoop/127.0.0.1:8020 trying to claim ACTIVE state with txid=56
2015-10-18 01:02:26,792 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid e3272006-5ef0-4545-8fb1-4cf4435a0d4d) service to CentOS.Hadoop/127.0.0.1:8020
2015-10-18 01:02:26,989 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 0 blocks total. Took 2 msec to generate and 194 msecs for RPC and NN processing.  Got back commands none
2015-10-18 01:02:27,003 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2015-10-18 01:02:27,003 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-10-18 01:02:27,006 INFO org.apache.hadoop.util.GSet: 0.5% max memory 966.7 MB = 4.8 MB
2015-10-18 01:02:27,006 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2015-10-18 01:02:27,006 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-340015217-127.0.0.1-1445079105718
2015-10-18 01:02:27,023 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-340015217-127.0.0.1-1445079105718 to blockPoolScannerMap, new size=1
2015-10-18 01:07:56,658 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
2015-10-18 01:08:00,649 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: CentOS.Hadoop/127.0.0.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 01:08:01,650 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: CentOS.Hadoop/127.0.0.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 01:08:02,652 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: CentOS.Hadoop/127.0.0.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 01:08:03,655 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: CentOS.Hadoop/127.0.0.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 01:08:04,658 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: CentOS.Hadoop/127.0.0.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 01:08:04,890 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-18 01:08:04,925 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
2015-10-18 01:08:54,683 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-18 01:08:55,517 WARN org.apache.hadoop.metrics2.impl.MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-datanode.properties,hadoop-metrics2.properties
2015-10-18 01:08:55,616 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-18 01:08:55,616 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-18 01:08:55,621 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: File descriptor passing is enabled.
2015-10-18 01:08:55,657 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is localhost.localdomain
2015-10-18 01:08:55,658 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-18 01:08:55,701 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-18 01:08:55,710 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-18 01:08:55,721 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-18 01:08:55,721 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Listening on UNIX domain socket: /var/run/hadoop-hdfs/dn.50010
2015-10-18 01:08:55,887 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-18 01:08:55,893 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-18 01:08:55,900 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-18 01:08:55,902 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-18 01:08:55,902 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-18 01:08:55,902 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-18 01:08:55,925 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-10-18 01:08:55,930 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-10-18 01:08:55,931 INFO org.mortbay.log: jetty-6.1.26
2015-10-18 01:08:56,253 INFO org.mortbay.log: Started SelectChannelConnector@0.0.0.0:50075
2015-10-18 01:08:56,516 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-18 01:08:56,539 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-18 01:08:56,557 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-18 01:08:56,570 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-18 01:08:56,582 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-18 01:08:56,643 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to CentOS.Hadoop/127.0.0.1:8020 starting to offer service
2015-10-18 01:08:56,658 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-18 01:08:56,674 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-18 01:08:56,970 INFO org.apache.hadoop.hdfs.server.common.Storage: Data-node version: -55 and name-node layout version: -55
2015-10-18 01:08:56,982 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/in_use.lock acquired by nodename 5968@CentOS.Hadoop
2015-10-18 01:08:57,142 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-340015217-127.0.0.1-1445079105718
2015-10-18 01:08:57,142 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled
2015-10-18 01:08:57,143 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-10-18 01:08:57,146 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=113353421;bpid=BP-340015217-127.0.0.1-1445079105718;lv=-55;nsInfo=lv=-55;cid=CID-1ea80fc5-7f68-47f8-8615-fee51275fc9c;nsid=113353421;c=0;bpid=BP-340015217-127.0.0.1-1445079105718;dnuuid=e3272006-5ef0-4545-8fb1-4cf4435a0d4d
2015-10-18 01:08:57,165 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current, StorageType: DISK
2015-10-18 01:08:57,300 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-10-18 01:08:57,312 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1445107296312 with interval 21600000
2015-10-18 01:08:57,314 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-340015217-127.0.0.1-1445079105718
2015-10-18 01:08:57,320 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-340015217-127.0.0.1-1445079105718 on volume /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current...
2015-10-18 01:08:57,346 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current/BP-340015217-127.0.0.1-1445079105718/current: 28672
2015-10-18 01:08:57,353 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-340015217-127.0.0.1-1445079105718 on /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current: 32ms
2015-10-18 01:08:57,354 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-340015217-127.0.0.1-1445079105718: 40ms
2015-10-18 01:08:57,354 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-340015217-127.0.0.1-1445079105718 on volume /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current...
2015-10-18 01:08:57,354 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-340015217-127.0.0.1-1445079105718 on volume /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current: 0ms
2015-10-18 01:08:57,354 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 0ms
2015-10-18 01:08:57,356 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid null) service to CentOS.Hadoop/127.0.0.1:8020 beginning handshake with NN
2015-10-18 01:08:57,438 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid null) service to CentOS.Hadoop/127.0.0.1:8020 successfully registered with NN
2015-10-18 01:08:57,438 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode CentOS.Hadoop/127.0.0.1:8020 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-10-18 01:08:57,607 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid e3272006-5ef0-4545-8fb1-4cf4435a0d4d) service to CentOS.Hadoop/127.0.0.1:8020 trying to claim ACTIVE state with txid=59
2015-10-18 01:08:57,607 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid e3272006-5ef0-4545-8fb1-4cf4435a0d4d) service to CentOS.Hadoop/127.0.0.1:8020
2015-10-18 01:08:57,716 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 0 blocks total. Took 7 msec to generate and 102 msecs for RPC and NN processing.  Got back commands none
2015-10-18 01:08:57,726 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2015-10-18 01:08:57,732 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-10-18 01:08:57,732 INFO org.apache.hadoop.util.GSet: 0.5% max memory 966.7 MB = 4.8 MB
2015-10-18 01:08:57,733 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2015-10-18 01:08:57,733 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-340015217-127.0.0.1-1445079105718
2015-10-18 01:08:57,742 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-340015217-127.0.0.1-1445079105718 to blockPoolScannerMap, new size=1
2015-10-18 01:11:55,802 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-18 01:11:55,807 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
2015-10-18 01:23:28,125 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-18 01:23:29,850 WARN org.apache.hadoop.metrics2.impl.MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-datanode.properties,hadoop-metrics2.properties
2015-10-18 01:23:30,003 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-18 01:23:30,003 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-18 01:23:30,008 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: File descriptor passing is enabled.
2015-10-18 01:23:30,015 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is localhost.localdomain
2015-10-18 01:23:30,016 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-18 01:23:30,120 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-18 01:23:30,137 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-18 01:23:30,141 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-18 01:23:30,141 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Listening on UNIX domain socket: /var/run/hadoop-hdfs/dn.50010
2015-10-18 01:23:30,463 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-18 01:23:30,466 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-18 01:23:30,474 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-18 01:23:30,475 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-18 01:23:30,476 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-18 01:23:30,476 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-18 01:23:30,499 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-10-18 01:23:30,508 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-10-18 01:23:30,508 INFO org.mortbay.log: jetty-6.1.26
2015-10-18 01:23:31,156 INFO org.mortbay.log: Started SelectChannelConnector@0.0.0.0:50075
2015-10-18 01:23:31,802 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-18 01:23:31,893 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-18 01:23:32,055 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-18 01:23:32,085 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-18 01:23:32,114 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-18 01:23:32,157 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to CentOS.Hadoop/127.0.0.1:8020 starting to offer service
2015-10-18 01:23:32,179 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-18 01:23:32,179 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-18 01:23:33,502 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: CentOS.Hadoop/127.0.0.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 01:23:34,503 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: CentOS.Hadoop/127.0.0.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 01:23:35,503 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: CentOS.Hadoop/127.0.0.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 01:23:36,504 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: CentOS.Hadoop/127.0.0.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 01:23:36,872 INFO org.apache.hadoop.hdfs.server.common.Storage: Data-node version: -55 and name-node layout version: -55
2015-10-18 01:23:36,890 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/in_use.lock acquired by nodename 2694@CentOS.Hadoop
2015-10-18 01:23:37,081 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-340015217-127.0.0.1-1445079105718
2015-10-18 01:23:37,082 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled
2015-10-18 01:23:37,083 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-10-18 01:23:37,085 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=113353421;bpid=BP-340015217-127.0.0.1-1445079105718;lv=-55;nsInfo=lv=-55;cid=CID-1ea80fc5-7f68-47f8-8615-fee51275fc9c;nsid=113353421;c=0;bpid=BP-340015217-127.0.0.1-1445079105718;dnuuid=e3272006-5ef0-4545-8fb1-4cf4435a0d4d
2015-10-18 01:23:37,111 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current, StorageType: DISK
2015-10-18 01:23:37,265 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-10-18 01:23:37,274 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1445115817274 with interval 21600000
2015-10-18 01:23:37,278 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-340015217-127.0.0.1-1445079105718
2015-10-18 01:23:37,281 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-340015217-127.0.0.1-1445079105718 on volume /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current...
2015-10-18 01:23:37,356 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-340015217-127.0.0.1-1445079105718 on /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current: 75ms
2015-10-18 01:23:37,356 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-340015217-127.0.0.1-1445079105718: 78ms
2015-10-18 01:23:37,359 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-340015217-127.0.0.1-1445079105718 on volume /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current...
2015-10-18 01:23:37,360 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-340015217-127.0.0.1-1445079105718 on volume /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current: 0ms
2015-10-18 01:23:37,360 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 4ms
2015-10-18 01:23:37,362 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid null) service to CentOS.Hadoop/127.0.0.1:8020 beginning handshake with NN
2015-10-18 01:23:37,463 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid null) service to CentOS.Hadoop/127.0.0.1:8020 successfully registered with NN
2015-10-18 01:23:37,463 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode CentOS.Hadoop/127.0.0.1:8020 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-10-18 01:23:37,672 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid e3272006-5ef0-4545-8fb1-4cf4435a0d4d) service to CentOS.Hadoop/127.0.0.1:8020 trying to claim ACTIVE state with txid=62
2015-10-18 01:23:37,672 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid e3272006-5ef0-4545-8fb1-4cf4435a0d4d) service to CentOS.Hadoop/127.0.0.1:8020
2015-10-18 01:23:37,790 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 0 blocks total. Took 1 msec to generate and 115 msecs for RPC and NN processing.  Got back commands none
2015-10-18 01:23:37,798 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2015-10-18 01:23:37,798 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-10-18 01:23:37,804 INFO org.apache.hadoop.util.GSet: 0.5% max memory 966.7 MB = 4.8 MB
2015-10-18 01:23:37,804 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2015-10-18 01:23:37,804 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-340015217-127.0.0.1-1445079105718
2015-10-18 01:23:37,807 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-340015217-127.0.0.1-1445079105718 to blockPoolScannerMap, new size=1
2015-10-18 01:24:56,473 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1084ms
2015-10-18 01:30:50,013 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-18 01:30:50,022 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
2015-10-18 01:54:27,182 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-18 01:54:28,839 WARN org.apache.hadoop.metrics2.impl.MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-datanode.properties,hadoop-metrics2.properties
2015-10-18 01:54:29,055 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-18 01:54:29,055 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-18 01:54:29,060 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: File descriptor passing is enabled.
2015-10-18 01:54:29,063 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is localhost.localdomain
2015-10-18 01:54:29,064 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-18 01:54:29,198 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-18 01:54:29,221 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-18 01:54:29,225 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-18 01:54:29,225 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Listening on UNIX domain socket: /var/run/hadoop-hdfs/dn.50010
2015-10-18 01:54:29,667 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-18 01:54:29,670 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-18 01:54:29,678 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-18 01:54:29,680 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-18 01:54:29,680 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-18 01:54:29,680 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-18 01:54:29,716 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-10-18 01:54:29,725 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-10-18 01:54:29,725 INFO org.mortbay.log: jetty-6.1.26
2015-10-18 01:54:30,354 INFO org.mortbay.log: Started SelectChannelConnector@0.0.0.0:50075
2015-10-18 01:54:30,944 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-18 01:54:30,972 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-18 01:54:31,082 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-18 01:54:31,124 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-18 01:54:31,183 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-18 01:54:31,233 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to CentOS.Hadoop/127.0.0.1:8020 starting to offer service
2015-10-18 01:54:31,259 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-18 01:54:31,258 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-18 01:54:32,603 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: CentOS.Hadoop/127.0.0.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 01:54:33,604 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: CentOS.Hadoop/127.0.0.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 01:54:34,605 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: CentOS.Hadoop/127.0.0.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 01:54:35,607 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: CentOS.Hadoop/127.0.0.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 01:54:36,041 INFO org.apache.hadoop.hdfs.server.common.Storage: Data-node version: -55 and name-node layout version: -55
2015-10-18 01:54:36,067 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/in_use.lock acquired by nodename 2670@CentOS.Hadoop
2015-10-18 01:54:36,255 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-340015217-127.0.0.1-1445079105718
2015-10-18 01:54:36,255 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled
2015-10-18 01:54:36,256 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-10-18 01:54:36,258 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=113353421;bpid=BP-340015217-127.0.0.1-1445079105718;lv=-55;nsInfo=lv=-55;cid=CID-1ea80fc5-7f68-47f8-8615-fee51275fc9c;nsid=113353421;c=0;bpid=BP-340015217-127.0.0.1-1445079105718;dnuuid=e3272006-5ef0-4545-8fb1-4cf4435a0d4d
2015-10-18 01:54:36,283 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current, StorageType: DISK
2015-10-18 01:54:36,442 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-10-18 01:54:36,450 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1445121117450 with interval 21600000
2015-10-18 01:54:36,454 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-340015217-127.0.0.1-1445079105718
2015-10-18 01:54:36,459 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-340015217-127.0.0.1-1445079105718 on volume /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current...
2015-10-18 01:54:36,527 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-340015217-127.0.0.1-1445079105718 on /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current: 67ms
2015-10-18 01:54:36,528 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-340015217-127.0.0.1-1445079105718: 74ms
2015-10-18 01:54:36,532 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-340015217-127.0.0.1-1445079105718 on volume /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current...
2015-10-18 01:54:36,532 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-340015217-127.0.0.1-1445079105718 on volume /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current: 0ms
2015-10-18 01:54:36,532 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 4ms
2015-10-18 01:54:36,534 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid null) service to CentOS.Hadoop/127.0.0.1:8020 beginning handshake with NN
2015-10-18 01:54:36,634 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid null) service to CentOS.Hadoop/127.0.0.1:8020 successfully registered with NN
2015-10-18 01:54:36,634 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode CentOS.Hadoop/127.0.0.1:8020 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-10-18 01:54:36,815 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid e3272006-5ef0-4545-8fb1-4cf4435a0d4d) service to CentOS.Hadoop/127.0.0.1:8020 trying to claim ACTIVE state with txid=65
2015-10-18 01:54:36,815 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid e3272006-5ef0-4545-8fb1-4cf4435a0d4d) service to CentOS.Hadoop/127.0.0.1:8020
2015-10-18 01:54:36,880 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 0 blocks total. Took 1 msec to generate and 64 msecs for RPC and NN processing.  Got back commands none
2015-10-18 01:54:36,884 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2015-10-18 01:54:36,884 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-10-18 01:54:36,885 INFO org.apache.hadoop.util.GSet: 0.5% max memory 966.7 MB = 4.8 MB
2015-10-18 01:54:36,885 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2015-10-18 01:54:36,885 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-340015217-127.0.0.1-1445079105718
2015-10-18 01:54:36,888 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-340015217-127.0.0.1-1445079105718 to blockPoolScannerMap, new size=1
2015-10-18 01:58:18,793 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-18 01:58:18,802 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
2015-10-18 17:36:19,783 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-18 17:36:21,443 WARN org.apache.hadoop.metrics2.impl.MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-datanode.properties,hadoop-metrics2.properties
2015-10-18 17:36:21,626 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-18 17:36:21,627 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-18 17:36:21,647 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: File descriptor passing is enabled.
2015-10-18 17:36:21,656 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is localhost.localdomain
2015-10-18 17:36:21,659 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-18 17:36:21,857 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-18 17:36:21,879 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-18 17:36:21,891 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-18 17:36:21,892 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Listening on UNIX domain socket: /var/run/hadoop-hdfs/dn.50010
2015-10-18 17:36:22,361 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-18 17:36:22,364 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-18 17:36:22,371 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-18 17:36:22,373 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-18 17:36:22,373 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-18 17:36:22,373 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-18 17:36:22,407 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-10-18 17:36:22,417 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-10-18 17:36:22,417 INFO org.mortbay.log: jetty-6.1.26
2015-10-18 17:36:23,113 INFO org.mortbay.log: Started SelectChannelConnector@0.0.0.0:50075
2015-10-18 17:36:23,849 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-18 17:36:23,898 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-18 17:36:23,998 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-18 17:36:24,036 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-18 17:36:24,072 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-18 17:36:24,117 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to CentOS.Hadoop/127.0.0.1:8020 starting to offer service
2015-10-18 17:36:24,148 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-18 17:36:24,147 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-18 17:36:25,500 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: CentOS.Hadoop/127.0.0.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 17:36:26,501 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: CentOS.Hadoop/127.0.0.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 17:36:27,503 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: CentOS.Hadoop/127.0.0.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 17:36:28,503 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: CentOS.Hadoop/127.0.0.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 17:36:29,505 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: CentOS.Hadoop/127.0.0.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 17:36:29,904 INFO org.apache.hadoop.hdfs.server.common.Storage: Data-node version: -55 and name-node layout version: -55
2015-10-18 17:36:29,917 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/in_use.lock acquired by nodename 2672@CentOS.Hadoop
2015-10-18 17:36:30,110 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-340015217-127.0.0.1-1445079105718
2015-10-18 17:36:30,110 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled
2015-10-18 17:36:30,111 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-10-18 17:36:30,113 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=113353421;bpid=BP-340015217-127.0.0.1-1445079105718;lv=-55;nsInfo=lv=-55;cid=CID-1ea80fc5-7f68-47f8-8615-fee51275fc9c;nsid=113353421;c=0;bpid=BP-340015217-127.0.0.1-1445079105718;dnuuid=e3272006-5ef0-4545-8fb1-4cf4435a0d4d
2015-10-18 17:36:30,146 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current, StorageType: DISK
2015-10-18 17:36:30,314 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-10-18 17:36:30,324 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1445173302324 with interval 21600000
2015-10-18 17:36:30,327 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-340015217-127.0.0.1-1445079105718
2015-10-18 17:36:30,331 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-340015217-127.0.0.1-1445079105718 on volume /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current...
2015-10-18 17:36:30,414 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-340015217-127.0.0.1-1445079105718 on /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current: 83ms
2015-10-18 17:36:30,414 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-340015217-127.0.0.1-1445079105718: 87ms
2015-10-18 17:36:30,419 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-340015217-127.0.0.1-1445079105718 on volume /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current...
2015-10-18 17:36:30,419 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-340015217-127.0.0.1-1445079105718 on volume /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current: 0ms
2015-10-18 17:36:30,419 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 5ms
2015-10-18 17:36:30,421 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid null) service to CentOS.Hadoop/127.0.0.1:8020 beginning handshake with NN
2015-10-18 17:36:30,533 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid null) service to CentOS.Hadoop/127.0.0.1:8020 successfully registered with NN
2015-10-18 17:36:30,533 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode CentOS.Hadoop/127.0.0.1:8020 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-10-18 17:36:30,768 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid e3272006-5ef0-4545-8fb1-4cf4435a0d4d) service to CentOS.Hadoop/127.0.0.1:8020 trying to claim ACTIVE state with txid=68
2015-10-18 17:36:30,768 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid e3272006-5ef0-4545-8fb1-4cf4435a0d4d) service to CentOS.Hadoop/127.0.0.1:8020
2015-10-18 17:36:30,881 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 0 blocks total. Took 1 msec to generate and 109 msecs for RPC and NN processing.  Got back commands none
2015-10-18 17:36:30,891 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2015-10-18 17:36:30,891 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-10-18 17:36:30,892 INFO org.apache.hadoop.util.GSet: 0.5% max memory 966.7 MB = 4.8 MB
2015-10-18 17:36:30,892 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2015-10-18 17:36:30,892 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-340015217-127.0.0.1-1445079105718
2015-10-18 17:36:30,896 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-340015217-127.0.0.1-1445079105718 to blockPoolScannerMap, new size=1
2015-10-18 17:38:45,472 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1071ms
2015-10-18 17:46:18,905 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 0 blocks total. Took 0 msec to generate and 3 msecs for RPC and NN processing.  Got back commands none
2015-10-18 18:50:31,995 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-18 18:50:32,047 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
2015-10-18 21:08:26,199 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-18 21:08:28,265 WARN org.apache.hadoop.metrics2.impl.MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-datanode.properties,hadoop-metrics2.properties
2015-10-18 21:08:28,540 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-18 21:08:28,541 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-18 21:08:28,555 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: File descriptor passing is enabled.
2015-10-18 21:08:28,560 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is localhost.localdomain
2015-10-18 21:08:28,561 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-18 21:08:28,696 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-18 21:08:28,718 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-18 21:08:28,723 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-18 21:08:28,723 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Listening on UNIX domain socket: /var/run/hadoop-hdfs/dn.50010
2015-10-18 21:08:30,165 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-18 21:08:30,169 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-18 21:08:30,178 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-18 21:08:30,180 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-18 21:08:30,180 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-18 21:08:30,180 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-18 21:08:30,252 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-10-18 21:08:30,265 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-10-18 21:08:30,265 INFO org.mortbay.log: jetty-6.1.26
2015-10-18 21:08:31,825 INFO org.mortbay.log: Started SelectChannelConnector@0.0.0.0:50075
2015-10-18 21:08:32,785 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-18 21:08:32,864 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-18 21:08:32,992 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-18 21:08:33,018 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-18 21:08:33,050 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-18 21:08:33,114 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to CentOS.Hadoop/127.0.0.1:8020 starting to offer service
2015-10-18 21:08:33,130 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-18 21:08:33,130 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-18 21:08:34,503 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: CentOS.Hadoop/127.0.0.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 21:08:35,504 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: CentOS.Hadoop/127.0.0.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 21:08:35,885 INFO org.apache.hadoop.hdfs.server.common.Storage: Data-node version: -55 and name-node layout version: -55
2015-10-18 21:08:35,939 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/in_use.lock acquired by nodename 2708@CentOS.Hadoop
2015-10-18 21:08:36,133 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-340015217-127.0.0.1-1445079105718
2015-10-18 21:08:36,133 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled
2015-10-18 21:08:36,135 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-10-18 21:08:36,137 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=113353421;bpid=BP-340015217-127.0.0.1-1445079105718;lv=-55;nsInfo=lv=-55;cid=CID-1ea80fc5-7f68-47f8-8615-fee51275fc9c;nsid=113353421;c=0;bpid=BP-340015217-127.0.0.1-1445079105718;dnuuid=e3272006-5ef0-4545-8fb1-4cf4435a0d4d
2015-10-18 21:08:36,163 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current, StorageType: DISK
2015-10-18 21:08:36,322 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-10-18 21:08:36,331 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1445180932331 with interval 21600000
2015-10-18 21:08:36,337 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-340015217-127.0.0.1-1445079105718
2015-10-18 21:08:36,338 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-340015217-127.0.0.1-1445079105718 on volume /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current...
2015-10-18 21:08:36,410 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-340015217-127.0.0.1-1445079105718 on /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current: 69ms
2015-10-18 21:08:36,410 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-340015217-127.0.0.1-1445079105718: 73ms
2015-10-18 21:08:36,412 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-340015217-127.0.0.1-1445079105718 on volume /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current...
2015-10-18 21:08:36,413 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-340015217-127.0.0.1-1445079105718 on volume /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current: 1ms
2015-10-18 21:08:36,413 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 3ms
2015-10-18 21:08:36,415 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid null) service to CentOS.Hadoop/127.0.0.1:8020 beginning handshake with NN
2015-10-18 21:08:36,517 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid null) service to CentOS.Hadoop/127.0.0.1:8020 successfully registered with NN
2015-10-18 21:08:36,518 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode CentOS.Hadoop/127.0.0.1:8020 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-10-18 21:08:36,759 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid e3272006-5ef0-4545-8fb1-4cf4435a0d4d) service to CentOS.Hadoop/127.0.0.1:8020 trying to claim ACTIVE state with txid=73
2015-10-18 21:08:36,759 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid e3272006-5ef0-4545-8fb1-4cf4435a0d4d) service to CentOS.Hadoop/127.0.0.1:8020
2015-10-18 21:08:36,824 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 0 blocks total. Took 2 msec to generate and 62 msecs for RPC and NN processing.  Got back commands none
2015-10-18 21:08:36,827 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2015-10-18 21:08:36,828 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-10-18 21:08:36,828 INFO org.apache.hadoop.util.GSet: 0.5% max memory 966.7 MB = 4.8 MB
2015-10-18 21:08:36,828 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2015-10-18 21:08:36,829 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-340015217-127.0.0.1-1445079105718
2015-10-18 21:08:36,832 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-340015217-127.0.0.1-1445079105718 to blockPoolScannerMap, new size=1
2015-10-18 22:04:14,261 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 0 blocks total. Took 0 msec to generate and 2 msecs for RPC and NN processing.  Got back commands none
2015-10-18 22:08:52,408 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-340015217-127.0.0.1-1445079105718 Total blocks: 0, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2015-10-18 23:18:23,725 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-340015217-127.0.0.1-1445079105718:blk_1073741825_1001 src: /127.0.0.1:38458 dest: /127.0.0.1:50010
2015-10-18 23:18:24,035 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:38458, dest: /127.0.0.1:50010, bytes: 2287829, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_846292499_1, offset: 0, srvID: e3272006-5ef0-4545-8fb1-4cf4435a0d4d, blockid: BP-340015217-127.0.0.1-1445079105718:blk_1073741825_1001, duration: 210047344
2015-10-18 23:18:24,038 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-340015217-127.0.0.1-1445079105718:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-18 23:18:29,343 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-340015217-127.0.0.1-1445079105718:blk_1073741825_1001
2015-10-18 23:18:37,641 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-340015217-127.0.0.1-1445079105718:blk_1073741826_1002 src: /127.0.0.1:38460 dest: /127.0.0.1:50010
2015-10-18 23:18:37,788 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:38460, dest: /127.0.0.1:50010, bytes: 102, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2001510166_1, offset: 0, srvID: e3272006-5ef0-4545-8fb1-4cf4435a0d4d, blockid: BP-340015217-127.0.0.1-1445079105718:blk_1073741826_1002, duration: 128577076
2015-10-18 23:18:37,788 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-340015217-127.0.0.1-1445079105718:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-18 23:18:39,391 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-340015217-127.0.0.1-1445079105718:blk_1073741826_1002
2015-10-18 23:18:45,320 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-340015217-127.0.0.1-1445079105718:blk_1073741827_1003 src: /127.0.0.1:38463 dest: /127.0.0.1:50010
2015-10-18 23:18:45,458 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:38463, dest: /127.0.0.1:50010, bytes: 5939, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1099224995_1, offset: 0, srvID: e3272006-5ef0-4545-8fb1-4cf4435a0d4d, blockid: BP-340015217-127.0.0.1-1445079105718:blk_1073741827_1003, duration: 120025456
2015-10-18 23:18:45,459 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-340015217-127.0.0.1-1445079105718:blk_1073741827_1003, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-18 23:18:49,397 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-340015217-127.0.0.1-1445079105718:blk_1073741827_1003
2015-10-19 00:00:06,948 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1766ms
2015-10-19 00:12:47,523 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: cliID: DFSClient_NONMAPREDUCE_2134218552_1, src: 127.0.0.1, dest: 127.0.0.1, op: REQUEST_SHORT_CIRCUIT_SHM, shmId: f1279d698068f76c8693f8bf157a64db, srvID: e3272006-5ef0-4545-8fb1-4cf4435a0d4d, success: true
2015-10-19 00:12:47,799 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: 127.0.0.1, dest: 127.0.0.1, op: REQUEST_SHORT_CIRCUIT_FDS, blockid: 1073741825, srvID: e3272006-5ef0-4545-8fb1-4cf4435a0d4d, success: true
2015-10-19 00:13:18,361 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: cliID: DFSClient_NONMAPREDUCE_1930751818_1, src: 127.0.0.1, dest: 127.0.0.1, op: REQUEST_SHORT_CIRCUIT_SHM, shmId: a9b4f9259241225accaae54133a56788, srvID: e3272006-5ef0-4545-8fb1-4cf4435a0d4d, success: true
2015-10-19 00:13:18,518 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: 127.0.0.1, dest: 127.0.0.1, op: REQUEST_SHORT_CIRCUIT_FDS, blockid: 1073741826, srvID: e3272006-5ef0-4545-8fb1-4cf4435a0d4d, success: true
2015-10-19 00:13:34,283 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: cliID: DFSClient_NONMAPREDUCE_-1629662749_1, src: 127.0.0.1, dest: 127.0.0.1, op: REQUEST_SHORT_CIRCUIT_SHM, shmId: 98f777d0fa3bee0de2fbce304f3e8dd9, srvID: e3272006-5ef0-4545-8fb1-4cf4435a0d4d, success: true
2015-10-19 00:13:34,469 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: 127.0.0.1, dest: 127.0.0.1, op: REQUEST_SHORT_CIRCUIT_FDS, blockid: 1073741827, srvID: e3272006-5ef0-4545-8fb1-4cf4435a0d4d, success: true
2015-10-19 00:31:32,378 WARN org.apache.hadoop.ipc.Server: IPC Server handler 3 on 50020, call org.apache.hadoop.hdfs.protocol.ClientDatanodeProtocol.getHdfsBlockLocations from 127.0.0.1:47823 Call#9 Retry#0: error: java.lang.UnsupportedOperationException: Datanode#getHdfsBlocksMetadata  is not enabled in datanode config
2015-10-19 00:31:34,881 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: cliID: DFSClient_NONMAPREDUCE_1686271395_1, src: 127.0.0.1, dest: 127.0.0.1, op: REQUEST_SHORT_CIRCUIT_SHM, shmId: 48ebd9347beae9831a6cb873d83ed892, srvID: e3272006-5ef0-4545-8fb1-4cf4435a0d4d, success: true
2015-10-19 00:31:34,935 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: 127.0.0.1, dest: 127.0.0.1, op: REQUEST_SHORT_CIRCUIT_FDS, blockid: 1073741827, srvID: e3272006-5ef0-4545-8fb1-4cf4435a0d4d, success: true
2015-10-19 00:47:46,133 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-19 00:47:46,138 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
2015-10-19 08:49:13,043 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-19 08:49:14,972 WARN org.apache.hadoop.metrics2.impl.MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-datanode.properties,hadoop-metrics2.properties
2015-10-19 08:49:15,146 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-19 08:49:15,147 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-19 08:49:15,151 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: File descriptor passing is enabled.
2015-10-19 08:49:15,162 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is localhost.localdomain
2015-10-19 08:49:15,163 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-19 08:49:15,293 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-19 08:49:15,316 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-19 08:49:15,320 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-19 08:49:15,320 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Listening on UNIX domain socket: /var/run/hadoop-hdfs/dn.50010
2015-10-19 08:49:15,488 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-19 08:49:15,491 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-19 08:49:15,499 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-19 08:49:15,501 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-19 08:49:15,501 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-19 08:49:15,501 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-19 08:49:15,536 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-10-19 08:49:15,555 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-10-19 08:49:15,555 INFO org.mortbay.log: jetty-6.1.26
2015-10-19 08:49:16,175 INFO org.mortbay.log: Started SelectChannelConnector@0.0.0.0:50075
2015-10-19 08:49:16,836 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-19 08:49:16,924 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-19 08:49:17,052 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-19 08:49:17,108 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-19 08:49:17,154 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-19 08:49:17,196 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to CentOS.Hadoop/127.0.0.1:8020 starting to offer service
2015-10-19 08:49:17,210 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-19 08:49:17,226 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-19 08:49:18,622 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: CentOS.Hadoop/127.0.0.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 08:49:19,623 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: CentOS.Hadoop/127.0.0.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 08:49:20,624 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: CentOS.Hadoop/127.0.0.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 08:49:21,625 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: CentOS.Hadoop/127.0.0.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 08:49:21,941 INFO org.apache.hadoop.hdfs.server.common.Storage: Data-node version: -55 and name-node layout version: -55
2015-10-19 08:49:21,955 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/in_use.lock acquired by nodename 2694@CentOS.Hadoop
2015-10-19 08:49:22,154 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-340015217-127.0.0.1-1445079105718
2015-10-19 08:49:22,155 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled
2015-10-19 08:49:22,156 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-10-19 08:49:22,158 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=113353421;bpid=BP-340015217-127.0.0.1-1445079105718;lv=-55;nsInfo=lv=-55;cid=CID-1ea80fc5-7f68-47f8-8615-fee51275fc9c;nsid=113353421;c=0;bpid=BP-340015217-127.0.0.1-1445079105718;dnuuid=e3272006-5ef0-4545-8fb1-4cf4435a0d4d
2015-10-19 08:49:22,184 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current, StorageType: DISK
2015-10-19 08:49:22,331 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-10-19 08:49:22,339 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1445230464339 with interval 21600000
2015-10-19 08:49:22,345 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-340015217-127.0.0.1-1445079105718
2015-10-19 08:49:22,345 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-340015217-127.0.0.1-1445079105718 on volume /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current...
2015-10-19 08:49:22,428 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-340015217-127.0.0.1-1445079105718 on /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current: 82ms
2015-10-19 08:49:22,428 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-340015217-127.0.0.1-1445079105718: 83ms
2015-10-19 08:49:22,431 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-340015217-127.0.0.1-1445079105718 on volume /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current...
2015-10-19 08:49:22,438 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-340015217-127.0.0.1-1445079105718 on volume /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current: 7ms
2015-10-19 08:49:22,438 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 10ms
2015-10-19 08:49:22,440 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid null) service to CentOS.Hadoop/127.0.0.1:8020 beginning handshake with NN
2015-10-19 08:49:22,553 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid null) service to CentOS.Hadoop/127.0.0.1:8020 successfully registered with NN
2015-10-19 08:49:22,553 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode CentOS.Hadoop/127.0.0.1:8020 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-10-19 08:49:22,785 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid e3272006-5ef0-4545-8fb1-4cf4435a0d4d) service to CentOS.Hadoop/127.0.0.1:8020 trying to claim ACTIVE state with txid=104
2015-10-19 08:49:22,785 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid e3272006-5ef0-4545-8fb1-4cf4435a0d4d) service to CentOS.Hadoop/127.0.0.1:8020
2015-10-19 08:49:22,874 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 3 blocks total. Took 1 msec to generate and 88 msecs for RPC and NN processing.  Got back commands none
2015-10-19 08:49:22,877 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2015-10-19 08:49:22,877 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-10-19 08:49:22,878 INFO org.apache.hadoop.util.GSet: 0.5% max memory 966.7 MB = 4.8 MB
2015-10-19 08:49:22,878 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2015-10-19 08:49:22,878 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-340015217-127.0.0.1-1445079105718
2015-10-19 08:49:22,882 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-340015217-127.0.0.1-1445079105718 to blockPoolScannerMap, new size=1
2015-10-19 08:50:36,019 WARN org.apache.hadoop.ipc.Server: IPC Server handler 9 on 50020, call org.apache.hadoop.hdfs.protocol.ClientDatanodeProtocol.getHdfsBlockLocations from 127.0.0.1:35586 Call#9 Retry#0: error: java.lang.UnsupportedOperationException: Datanode#getHdfsBlocksMetadata  is not enabled in datanode config
2015-10-19 09:37:32,618 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: cliID: DFSClient_NONMAPREDUCE_1059061389_1, src: 127.0.0.1, dest: 127.0.0.1, op: REQUEST_SHORT_CIRCUIT_SHM, shmId: a11c519ff9e1fb9f762e88a36aaae8c0, srvID: e3272006-5ef0-4545-8fb1-4cf4435a0d4d, success: true
2015-10-19 09:37:32,859 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: 127.0.0.1, dest: 127.0.0.1, op: REQUEST_SHORT_CIRCUIT_FDS, blockid: 1073741827, srvID: e3272006-5ef0-4545-8fb1-4cf4435a0d4d, success: true
2015-10-19 11:54:24,445 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-340015217-127.0.0.1-1445079105718 Total blocks: 3, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2015-10-19 12:35:41,391 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1177ms
2015-10-19 12:46:43,726 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 3 blocks total. Took 1 msec to generate and 268 msecs for RPC and NN processing.  Got back commands none
2015-10-19 14:04:12,251 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeCommand action : DNA_REGISTER from CentOS.Hadoop/127.0.0.1:8020 with active state
2015-10-19 14:04:12,852 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid e3272006-5ef0-4545-8fb1-4cf4435a0d4d) service to CentOS.Hadoop/127.0.0.1:8020 beginning handshake with NN
2015-10-19 14:04:42,869 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid e3272006-5ef0-4545-8fb1-4cf4435a0d4d) service to CentOS.Hadoop/127.0.0.1:8020 successfully registered with NN
2015-10-19 14:04:43,107 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Took 30856ms to process 1 commands from NN
2015-10-19 14:04:46,363 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 3 blocks total. Took 0 msec to generate and 3256 msecs for RPC and NN processing.  Got back commands none
2015-10-19 15:14:44,050 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
2015-10-19 15:14:44,115 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-19 15:14:45,817 WARN org.apache.hadoop.metrics2.impl.MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-datanode.properties,hadoop-metrics2.properties
2015-10-19 15:14:46,024 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-19 15:14:46,024 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-19 15:14:46,029 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: File descriptor passing is enabled.
2015-10-19 15:14:46,032 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is localhost.localdomain
2015-10-19 15:14:46,033 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-19 15:14:46,246 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-19 15:14:46,271 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-19 15:14:46,276 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-19 15:14:46,276 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Listening on UNIX domain socket: /var/run/hadoop-hdfs/dn.50010
2015-10-19 15:14:46,642 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-19 15:14:46,645 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-19 15:14:46,654 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-19 15:14:46,655 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-19 15:14:46,656 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-19 15:14:46,656 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-19 15:14:46,689 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-10-19 15:14:46,698 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-10-19 15:14:46,698 INFO org.mortbay.log: jetty-6.1.26
2015-10-19 15:14:47,396 INFO org.mortbay.log: Started SelectChannelConnector@0.0.0.0:50075
2015-10-19 15:14:48,001 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-19 15:14:48,028 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-19 15:14:48,159 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-19 15:14:48,250 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-19 15:14:48,299 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-19 15:14:48,339 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to CentOS.Hadoop/127.0.0.1:8020 starting to offer service
2015-10-19 15:14:48,349 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-19 15:14:48,349 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-19 15:14:49,868 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: CentOS.Hadoop/127.0.0.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 15:14:50,869 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: CentOS.Hadoop/127.0.0.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 15:14:51,870 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: CentOS.Hadoop/127.0.0.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 15:14:52,872 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: CentOS.Hadoop/127.0.0.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 15:14:53,263 INFO org.apache.hadoop.hdfs.server.common.Storage: Data-node version: -55 and name-node layout version: -55
2015-10-19 15:14:53,283 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/in_use.lock acquired by nodename 2759@CentOS.Hadoop
2015-10-19 15:14:53,521 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-340015217-127.0.0.1-1445079105718
2015-10-19 15:14:53,521 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled
2015-10-19 15:14:53,522 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-10-19 15:14:53,524 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=113353421;bpid=BP-340015217-127.0.0.1-1445079105718;lv=-55;nsInfo=lv=-55;cid=CID-1ea80fc5-7f68-47f8-8615-fee51275fc9c;nsid=113353421;c=0;bpid=BP-340015217-127.0.0.1-1445079105718;dnuuid=e3272006-5ef0-4545-8fb1-4cf4435a0d4d
2015-10-19 15:14:53,554 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current, StorageType: DISK
2015-10-19 15:14:53,727 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-10-19 15:14:53,736 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1445256308736 with interval 21600000
2015-10-19 15:14:53,741 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-340015217-127.0.0.1-1445079105718
2015-10-19 15:14:53,744 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-340015217-127.0.0.1-1445079105718 on volume /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current...
2015-10-19 15:14:53,806 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-340015217-127.0.0.1-1445079105718 on /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current: 62ms
2015-10-19 15:14:53,807 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-340015217-127.0.0.1-1445079105718: 66ms
2015-10-19 15:14:53,808 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-340015217-127.0.0.1-1445079105718 on volume /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current...
2015-10-19 15:14:53,815 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-340015217-127.0.0.1-1445079105718 on volume /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current: 7ms
2015-10-19 15:14:53,815 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 7ms
2015-10-19 15:14:53,817 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid null) service to CentOS.Hadoop/127.0.0.1:8020 beginning handshake with NN
2015-10-19 15:14:53,926 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid null) service to CentOS.Hadoop/127.0.0.1:8020 successfully registered with NN
2015-10-19 15:14:53,926 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode CentOS.Hadoop/127.0.0.1:8020 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-10-19 15:14:54,134 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid e3272006-5ef0-4545-8fb1-4cf4435a0d4d) service to CentOS.Hadoop/127.0.0.1:8020 trying to claim ACTIVE state with txid=124
2015-10-19 15:14:54,134 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid e3272006-5ef0-4545-8fb1-4cf4435a0d4d) service to CentOS.Hadoop/127.0.0.1:8020
2015-10-19 15:14:54,287 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 3 blocks total. Took 2 msec to generate and 150 msecs for RPC and NN processing.  Got back commands none
2015-10-19 15:14:54,291 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2015-10-19 15:14:54,291 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-10-19 15:14:54,292 INFO org.apache.hadoop.util.GSet: 0.5% max memory 966.7 MB = 4.8 MB
2015-10-19 15:14:54,292 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2015-10-19 15:14:54,293 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-340015217-127.0.0.1-1445079105718
2015-10-19 15:14:54,299 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-340015217-127.0.0.1-1445079105718 to blockPoolScannerMap, new size=1
2015-10-19 15:16:05,414 WARN org.apache.hadoop.ipc.Server: IPC Server handler 0 on 50020, call org.apache.hadoop.hdfs.protocol.ClientDatanodeProtocol.getHdfsBlockLocations from 127.0.0.1:60980 Call#9 Retry#0: error: java.lang.UnsupportedOperationException: Datanode#getHdfsBlocksMetadata  is not enabled in datanode config
2015-10-19 15:17:20,990 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-340015217-127.0.0.1-1445079105718:blk_1073741829_1005 src: /127.0.0.1:41233 dest: /127.0.0.1:50010
2015-10-19 15:17:21,249 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:41233, dest: /127.0.0.1:50010, bytes: 381, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-506420409_1, offset: 0, srvID: e3272006-5ef0-4545-8fb1-4cf4435a0d4d, blockid: BP-340015217-127.0.0.1-1445079105718:blk_1073741829_1005, duration: 142969121
2015-10-19 15:17:21,249 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-340015217-127.0.0.1-1445079105718:blk_1073741829_1005, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-19 15:17:24,453 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-340015217-127.0.0.1-1445079105718:blk_1073741829_1005
2015-10-19 15:18:50,442 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:50010, dest: /127.0.0.1:41238, bytes: 385, op: HDFS_READ, cliID: DFSClient_NONMAPREDUCE_1826721202_11, offset: 0, srvID: e3272006-5ef0-4545-8fb1-4cf4435a0d4d, blockid: BP-340015217-127.0.0.1-1445079105718:blk_1073741829_1005, duration: 34309200
2015-10-19 15:25:53,952 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:50010, dest: /127.0.0.1:41250, bytes: 385, op: HDFS_READ, cliID: DFSClient_NONMAPREDUCE_1826721202_11, offset: 0, srvID: e3272006-5ef0-4545-8fb1-4cf4435a0d4d, blockid: BP-340015217-127.0.0.1-1445079105718:blk_1073741829_1005, duration: 11942477
2015-10-19 16:29:38,151 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2641ms
2015-10-19 16:36:43,605 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-340015217-127.0.0.1-1445079105718:blk_1073741830_1006 src: /127.0.0.1:41359 dest: /127.0.0.1:50010
2015-10-19 16:36:43,713 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:41359, dest: /127.0.0.1:50010, bytes: 673, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1826721202_11, offset: 0, srvID: e3272006-5ef0-4545-8fb1-4cf4435a0d4d, blockid: BP-340015217-127.0.0.1-1445079105718:blk_1073741830_1006, duration: 40897023
2015-10-19 16:36:43,713 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-340015217-127.0.0.1-1445079105718:blk_1073741830_1006, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-19 16:36:44,792 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-340015217-127.0.0.1-1445079105718:blk_1073741830_1006
2015-10-19 16:38:35,364 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: cliID: DFSClient_NONMAPREDUCE_621121117_1, src: 127.0.0.1, dest: 127.0.0.1, op: REQUEST_SHORT_CIRCUIT_SHM, shmId: 68b764e2fba52d4cbc21684498fb7527, srvID: e3272006-5ef0-4545-8fb1-4cf4435a0d4d, success: true
2015-10-19 16:38:35,869 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: 127.0.0.1, dest: 127.0.0.1, op: REQUEST_SHORT_CIRCUIT_FDS, blockid: 1073741830, srvID: e3272006-5ef0-4545-8fb1-4cf4435a0d4d, success: true
2015-10-19 17:00:59,690 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 5 blocks total. Took 1 msec to generate and 104 msecs for RPC and NN processing.  Got back commands none
2015-10-19 17:18:04,095 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1260ms
2015-10-19 17:51:35,655 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-19 17:51:35,709 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
2015-10-20 09:45:57,816 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-20 09:45:59,578 WARN org.apache.hadoop.metrics2.impl.MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-datanode.properties,hadoop-metrics2.properties
2015-10-20 09:45:59,752 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-20 09:45:59,752 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-20 09:45:59,756 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: File descriptor passing is enabled.
2015-10-20 09:45:59,768 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is localhost.localdomain
2015-10-20 09:45:59,768 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-20 09:45:59,908 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-20 09:45:59,930 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-20 09:45:59,934 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-20 09:45:59,934 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Listening on UNIX domain socket: /var/run/hadoop-hdfs/dn.50010
2015-10-20 09:46:00,197 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-20 09:46:00,200 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-20 09:46:00,208 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-20 09:46:00,210 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-20 09:46:00,210 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-20 09:46:00,210 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-20 09:46:00,250 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-10-20 09:46:00,261 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-10-20 09:46:00,261 INFO org.mortbay.log: jetty-6.1.26
2015-10-20 09:46:00,798 INFO org.mortbay.log: Started SelectChannelConnector@0.0.0.0:50075
2015-10-20 09:46:01,379 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-20 09:46:01,412 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-20 09:46:01,475 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-20 09:46:01,490 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-20 09:46:01,553 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-20 09:46:01,592 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to CentOS.Hadoop/127.0.0.1:8020 starting to offer service
2015-10-20 09:46:01,595 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-20 09:46:01,596 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-20 09:46:02,946 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: CentOS.Hadoop/127.0.0.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-20 09:46:03,947 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: CentOS.Hadoop/127.0.0.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-20 09:46:04,948 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: CentOS.Hadoop/127.0.0.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-20 09:46:05,949 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: CentOS.Hadoop/127.0.0.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-20 09:46:06,950 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: CentOS.Hadoop/127.0.0.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-20 09:46:07,265 INFO org.apache.hadoop.hdfs.server.common.Storage: Data-node version: -55 and name-node layout version: -55
2015-10-20 09:46:07,276 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/in_use.lock acquired by nodename 2689@CentOS.Hadoop
2015-10-20 09:46:07,473 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-340015217-127.0.0.1-1445079105718
2015-10-20 09:46:07,473 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled
2015-10-20 09:46:07,475 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-10-20 09:46:07,477 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=113353421;bpid=BP-340015217-127.0.0.1-1445079105718;lv=-55;nsInfo=lv=-55;cid=CID-1ea80fc5-7f68-47f8-8615-fee51275fc9c;nsid=113353421;c=0;bpid=BP-340015217-127.0.0.1-1445079105718;dnuuid=e3272006-5ef0-4545-8fb1-4cf4435a0d4d
2015-10-20 09:46:07,503 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current, StorageType: DISK
2015-10-20 09:46:07,658 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-10-20 09:46:07,666 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1445319373666 with interval 21600000
2015-10-20 09:46:07,670 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-340015217-127.0.0.1-1445079105718
2015-10-20 09:46:07,674 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-340015217-127.0.0.1-1445079105718 on volume /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current...
2015-10-20 09:46:07,759 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-340015217-127.0.0.1-1445079105718 on /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current: 85ms
2015-10-20 09:46:07,759 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-340015217-127.0.0.1-1445079105718: 88ms
2015-10-20 09:46:07,760 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-340015217-127.0.0.1-1445079105718 on volume /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current...
2015-10-20 09:46:07,766 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-340015217-127.0.0.1-1445079105718 on volume /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current: 6ms
2015-10-20 09:46:07,766 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 7ms
2015-10-20 09:46:07,768 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid null) service to CentOS.Hadoop/127.0.0.1:8020 beginning handshake with NN
2015-10-20 09:46:07,888 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid null) service to CentOS.Hadoop/127.0.0.1:8020 successfully registered with NN
2015-10-20 09:46:07,888 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode CentOS.Hadoop/127.0.0.1:8020 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-10-20 09:46:08,087 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid e3272006-5ef0-4545-8fb1-4cf4435a0d4d) service to CentOS.Hadoop/127.0.0.1:8020 trying to claim ACTIVE state with txid=154
2015-10-20 09:46:08,087 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid e3272006-5ef0-4545-8fb1-4cf4435a0d4d) service to CentOS.Hadoop/127.0.0.1:8020
2015-10-20 09:46:08,214 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 5 blocks total. Took 2 msec to generate and 125 msecs for RPC and NN processing.  Got back commands none
2015-10-20 09:46:08,217 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2015-10-20 09:46:08,217 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-10-20 09:46:08,218 INFO org.apache.hadoop.util.GSet: 0.5% max memory 966.7 MB = 4.8 MB
2015-10-20 09:46:08,218 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2015-10-20 09:46:08,219 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-340015217-127.0.0.1-1445079105718
2015-10-20 09:46:08,222 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-340015217-127.0.0.1-1445079105718 to blockPoolScannerMap, new size=1
2015-10-20 09:47:22,834 WARN org.apache.hadoop.ipc.Server: IPC Server handler 0 on 50020, call org.apache.hadoop.hdfs.protocol.ClientDatanodeProtocol.getHdfsBlockLocations from 127.0.0.1:55158 Call#9 Retry#0: error: java.lang.UnsupportedOperationException: Datanode#getHdfsBlocksMetadata  is not enabled in datanode config
2015-10-20 09:52:49,880 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: cliID: DFSClient_NONMAPREDUCE_2068357949_1, src: 127.0.0.1, dest: 127.0.0.1, op: REQUEST_SHORT_CIRCUIT_SHM, shmId: fbcb40480fb101cb33adb3eafd9930db, srvID: e3272006-5ef0-4545-8fb1-4cf4435a0d4d, success: true
2015-10-20 09:52:49,950 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: 127.0.0.1, dest: 127.0.0.1, op: REQUEST_SHORT_CIRCUIT_FDS, blockid: 1073741825, srvID: e3272006-5ef0-4545-8fb1-4cf4435a0d4d, success: true
2015-10-20 09:55:02,677 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741825_1001 file /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current/BP-340015217-127.0.0.1-1445079105718/current/finalized/blk_1073741825 for deletion
2015-10-20 09:55:02,681 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-340015217-127.0.0.1-1445079105718 blk_1073741825_1001 file /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current/BP-340015217-127.0.0.1-1445079105718/current/finalized/blk_1073741825
2015-10-20 09:55:27,321 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-340015217-127.0.0.1-1445079105718:blk_1073741831_1007 src: /127.0.0.1:53266 dest: /127.0.0.1:50010
2015-10-20 09:55:27,578 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:53266, dest: /127.0.0.1:50010, bytes: 2306164, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_428961487_1, offset: 0, srvID: e3272006-5ef0-4545-8fb1-4cf4435a0d4d, blockid: BP-340015217-127.0.0.1-1445079105718:blk_1073741831_1007, duration: 205925719
2015-10-20 09:55:27,578 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-340015217-127.0.0.1-1445079105718:blk_1073741831_1007, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-20 09:55:28,597 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-340015217-127.0.0.1-1445079105718:blk_1073741831_1007
2015-10-20 09:55:36,249 WARN org.apache.hadoop.ipc.Server: IPC Server handler 3 on 50020, call org.apache.hadoop.hdfs.protocol.ClientDatanodeProtocol.getHdfsBlockLocations from 127.0.0.1:55198 Call#29 Retry#0: error: java.lang.UnsupportedOperationException: Datanode#getHdfsBlocksMetadata  is not enabled in datanode config
2015-10-20 09:55:41,680 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: 127.0.0.1, dest: 127.0.0.1, op: REQUEST_SHORT_CIRCUIT_FDS, blockid: 1073741831, srvID: e3272006-5ef0-4545-8fb1-4cf4435a0d4d, success: true
2015-10-20 09:57:02,667 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741831_1007 file /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current/BP-340015217-127.0.0.1-1445079105718/current/finalized/blk_1073741831 for deletion
2015-10-20 09:57:02,670 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-340015217-127.0.0.1-1445079105718 blk_1073741831_1007 file /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current/BP-340015217-127.0.0.1-1445079105718/current/finalized/blk_1073741831
2015-10-20 09:57:04,439 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-340015217-127.0.0.1-1445079105718:blk_1073741832_1008 src: /127.0.0.1:53289 dest: /127.0.0.1:50010
2015-10-20 09:57:04,600 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:53289, dest: /127.0.0.1:50010, bytes: 2290741, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1448871477_1, offset: 0, srvID: e3272006-5ef0-4545-8fb1-4cf4435a0d4d, blockid: BP-340015217-127.0.0.1-1445079105718:blk_1073741832_1008, duration: 152594818
2015-10-20 09:57:04,600 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-340015217-127.0.0.1-1445079105718:blk_1073741832_1008, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-20 09:57:10,615 WARN org.apache.hadoop.ipc.Server: IPC Server handler 4 on 50020, call org.apache.hadoop.hdfs.protocol.ClientDatanodeProtocol.getHdfsBlockLocations from 127.0.0.1:55218 Call#59 Retry#0: error: java.lang.UnsupportedOperationException: Datanode#getHdfsBlocksMetadata  is not enabled in datanode config
2015-10-20 09:57:10,644 WARN org.apache.hadoop.ipc.Server: IPC Server handler 7 on 50020, call org.apache.hadoop.hdfs.protocol.ClientDatanodeProtocol.getHdfsBlockLocations from 127.0.0.1:55218 Call#69 Retry#0: error: java.lang.UnsupportedOperationException: Datanode#getHdfsBlocksMetadata  is not enabled in datanode config
2015-10-20 09:57:10,655 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-340015217-127.0.0.1-1445079105718:blk_1073741832_1008
2015-10-20 09:57:10,825 WARN org.apache.hadoop.ipc.Server: IPC Server handler 8 on 50020, call org.apache.hadoop.hdfs.protocol.ClientDatanodeProtocol.getHdfsBlockLocations from 127.0.0.1:55218 Call#79 Retry#0: error: java.lang.UnsupportedOperationException: Datanode#getHdfsBlocksMetadata  is not enabled in datanode config
2015-10-20 09:57:13,515 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: 127.0.0.1, dest: 127.0.0.1, op: REQUEST_SHORT_CIRCUIT_FDS, blockid: 1073741832, srvID: e3272006-5ef0-4545-8fb1-4cf4435a0d4d, success: true
2015-10-20 10:02:17,760 WARN org.apache.hadoop.ipc.Server: IPC Server handler 3 on 50020, call org.apache.hadoop.hdfs.protocol.ClientDatanodeProtocol.getHdfsBlockLocations from 127.0.0.1:55251 Call#89 Retry#0: error: java.lang.UnsupportedOperationException: Datanode#getHdfsBlocksMetadata  is not enabled in datanode config
2015-10-20 10:04:35,950 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741832_1008 file /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current/BP-340015217-127.0.0.1-1445079105718/current/finalized/blk_1073741832 for deletion
2015-10-20 10:04:35,967 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-340015217-127.0.0.1-1445079105718 blk_1073741832_1008 file /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current/BP-340015217-127.0.0.1-1445079105718/current/finalized/blk_1073741832
2015-10-20 10:04:36,833 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-340015217-127.0.0.1-1445079105718:blk_1073741833_1009 src: /127.0.0.1:53348 dest: /127.0.0.1:50010
2015-10-20 10:04:37,003 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:53348, dest: /127.0.0.1:50010, bytes: 2283121, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-733909909_1, offset: 0, srvID: e3272006-5ef0-4545-8fb1-4cf4435a0d4d, blockid: BP-340015217-127.0.0.1-1445079105718:blk_1073741833_1009, duration: 161643946
2015-10-20 10:04:37,003 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-340015217-127.0.0.1-1445079105718:blk_1073741833_1009, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-20 10:04:43,771 WARN org.apache.hadoop.ipc.Server: IPC Server handler 9 on 50020, call org.apache.hadoop.hdfs.protocol.ClientDatanodeProtocol.getHdfsBlockLocations from 127.0.0.1:55277 Call#99 Retry#0: error: java.lang.UnsupportedOperationException: Datanode#getHdfsBlocksMetadata  is not enabled in datanode config
2015-10-20 10:04:43,797 WARN org.apache.hadoop.ipc.Server: IPC Server handler 1 on 50020, call org.apache.hadoop.hdfs.protocol.ClientDatanodeProtocol.getHdfsBlockLocations from 127.0.0.1:55277 Call#109 Retry#0: error: java.lang.UnsupportedOperationException: Datanode#getHdfsBlocksMetadata  is not enabled in datanode config
2015-10-20 10:04:44,077 WARN org.apache.hadoop.ipc.Server: IPC Server handler 5 on 50020, call org.apache.hadoop.hdfs.protocol.ClientDatanodeProtocol.getHdfsBlockLocations from 127.0.0.1:55277 Call#119 Retry#0: error: java.lang.UnsupportedOperationException: Datanode#getHdfsBlocksMetadata  is not enabled in datanode config
2015-10-20 10:05:00,384 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: 127.0.0.1, dest: 127.0.0.1, op: REQUEST_SHORT_CIRCUIT_FDS, blockid: 1073741833, srvID: e3272006-5ef0-4545-8fb1-4cf4435a0d4d, success: true
2015-10-20 10:05:53,979 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741827_1003 file /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current/BP-340015217-127.0.0.1-1445079105718/current/finalized/blk_1073741827 for deletion
2015-10-20 10:05:54,017 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-340015217-127.0.0.1-1445079105718 blk_1073741827_1003 file /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current/BP-340015217-127.0.0.1-1445079105718/current/finalized/blk_1073741827
2015-10-20 10:06:03,004 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741826_1002 file /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current/BP-340015217-127.0.0.1-1445079105718/current/finalized/blk_1073741826 for deletion
2015-10-20 10:06:03,005 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-340015217-127.0.0.1-1445079105718 blk_1073741826_1002 file /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current/BP-340015217-127.0.0.1-1445079105718/current/finalized/blk_1073741826
2015-10-20 10:06:24,066 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-340015217-127.0.0.1-1445079105718:blk_1073741834_1010 src: /127.0.0.1:53372 dest: /127.0.0.1:50010
2015-10-20 10:06:24,203 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:53372, dest: /127.0.0.1:50010, bytes: 87, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1673715189_1, offset: 0, srvID: e3272006-5ef0-4545-8fb1-4cf4435a0d4d, blockid: BP-340015217-127.0.0.1-1445079105718:blk_1073741834_1010, duration: 130940916
2015-10-20 10:06:24,203 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-340015217-127.0.0.1-1445079105718:blk_1073741834_1010, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-20 10:06:44,072 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-340015217-127.0.0.1-1445079105718:blk_1073741835_1011 src: /127.0.0.1:53374 dest: /127.0.0.1:50010
2015-10-20 10:06:44,224 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:53374, dest: /127.0.0.1:50010, bytes: 5920, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2110066043_1, offset: 0, srvID: e3272006-5ef0-4545-8fb1-4cf4435a0d4d, blockid: BP-340015217-127.0.0.1-1445079105718:blk_1073741835_1011, duration: 141896112
2015-10-20 10:06:44,224 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-340015217-127.0.0.1-1445079105718:blk_1073741835_1011, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-20 10:07:01,655 WARN org.apache.hadoop.ipc.Server: IPC Server handler 4 on 50020, call org.apache.hadoop.hdfs.protocol.ClientDatanodeProtocol.getHdfsBlockLocations from 127.0.0.1:55303 Call#129 Retry#0: error: java.lang.UnsupportedOperationException: Datanode#getHdfsBlocksMetadata  is not enabled in datanode config
2015-10-20 10:07:01,694 WARN org.apache.hadoop.ipc.Server: IPC Server handler 7 on 50020, call org.apache.hadoop.hdfs.protocol.ClientDatanodeProtocol.getHdfsBlockLocations from 127.0.0.1:55303 Call#139 Retry#0: error: java.lang.UnsupportedOperationException: Datanode#getHdfsBlocksMetadata  is not enabled in datanode config
2015-10-20 10:07:01,857 WARN org.apache.hadoop.ipc.Server: IPC Server handler 8 on 50020, call org.apache.hadoop.hdfs.protocol.ClientDatanodeProtocol.getHdfsBlockLocations from 127.0.0.1:55303 Call#149 Retry#0: error: java.lang.UnsupportedOperationException: Datanode#getHdfsBlocksMetadata  is not enabled in datanode config
2015-10-20 10:07:08,496 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: 127.0.0.1, dest: 127.0.0.1, op: REQUEST_SHORT_CIRCUIT_FDS, blockid: 1073741834, srvID: e3272006-5ef0-4545-8fb1-4cf4435a0d4d, success: true
2015-10-20 10:07:38,196 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: 127.0.0.1, dest: 127.0.0.1, op: REQUEST_SHORT_CIRCUIT_FDS, blockid: 1073741835, srvID: e3272006-5ef0-4545-8fb1-4cf4435a0d4d, success: true
2015-10-20 10:10:11,157 WARN org.apache.hadoop.ipc.Server: IPC Server handler 8 on 50020, call org.apache.hadoop.hdfs.protocol.ClientDatanodeProtocol.getHdfsBlockLocations from 127.0.0.1:55322 Call#159 Retry#0: error: java.lang.UnsupportedOperationException: Datanode#getHdfsBlocksMetadata  is not enabled in datanode config
2015-10-20 11:08:34,123 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-20 11:08:35,774 WARN org.apache.hadoop.metrics2.impl.MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-datanode.properties,hadoop-metrics2.properties
2015-10-20 11:08:36,008 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-20 11:08:36,008 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-20 11:08:36,015 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: File descriptor passing is enabled.
2015-10-20 11:08:36,020 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is localhost.localdomain
2015-10-20 11:08:36,021 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-20 11:08:36,127 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-20 11:08:36,141 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-20 11:08:36,145 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-20 11:08:36,145 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Listening on UNIX domain socket: /var/run/hadoop-hdfs/dn.50010
2015-10-20 11:08:36,394 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-20 11:08:36,397 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-20 11:08:36,405 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-20 11:08:36,407 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-20 11:08:36,407 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-20 11:08:36,407 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-20 11:08:36,445 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-10-20 11:08:36,450 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-10-20 11:08:36,450 INFO org.mortbay.log: jetty-6.1.26
2015-10-20 11:08:36,985 INFO org.mortbay.log: Started SelectChannelConnector@0.0.0.0:50075
2015-10-20 11:08:37,398 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-20 11:08:37,415 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-20 11:08:37,485 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-20 11:08:37,500 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-20 11:08:37,532 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-20 11:08:37,574 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to CentOS.Hadoop/127.0.0.1:8020 starting to offer service
2015-10-20 11:08:37,584 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-20 11:08:37,585 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-20 11:08:37,977 INFO org.apache.hadoop.hdfs.server.common.Storage: Data-node version: -55 and name-node layout version: -55
2015-10-20 11:08:38,017 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/in_use.lock acquired by nodename 6703@CentOS.Hadoop
2015-10-20 11:08:38,318 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-340015217-127.0.0.1-1445079105718
2015-10-20 11:08:38,318 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled
2015-10-20 11:08:38,319 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-10-20 11:08:38,320 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=113353421;bpid=BP-340015217-127.0.0.1-1445079105718;lv=-55;nsInfo=lv=-55;cid=CID-1ea80fc5-7f68-47f8-8615-fee51275fc9c;nsid=113353421;c=0;bpid=BP-340015217-127.0.0.1-1445079105718;dnuuid=e3272006-5ef0-4545-8fb1-4cf4435a0d4d
2015-10-20 11:08:38,360 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current, StorageType: DISK
2015-10-20 11:08:38,529 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-10-20 11:08:38,542 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1445330903542 with interval 21600000
2015-10-20 11:08:38,546 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-340015217-127.0.0.1-1445079105718
2015-10-20 11:08:38,552 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-340015217-127.0.0.1-1445079105718 on volume /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current...
2015-10-20 11:08:38,586 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current/BP-340015217-127.0.0.1-1445079105718/current: 2375680
2015-10-20 11:08:38,591 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-340015217-127.0.0.1-1445079105718 on /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current: 39ms
2015-10-20 11:08:38,591 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-340015217-127.0.0.1-1445079105718: 45ms
2015-10-20 11:08:38,596 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-340015217-127.0.0.1-1445079105718 on volume /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current...
2015-10-20 11:08:38,603 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-340015217-127.0.0.1-1445079105718 on volume /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current: 7ms
2015-10-20 11:08:38,603 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 12ms
2015-10-20 11:08:38,607 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid null) service to CentOS.Hadoop/127.0.0.1:8020 beginning handshake with NN
2015-10-20 11:08:38,647 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid null) service to CentOS.Hadoop/127.0.0.1:8020 successfully registered with NN
2015-10-20 11:08:38,647 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode CentOS.Hadoop/127.0.0.1:8020 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-10-20 11:08:38,770 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid e3272006-5ef0-4545-8fb1-4cf4435a0d4d) service to CentOS.Hadoop/127.0.0.1:8020 trying to claim ACTIVE state with txid=196
2015-10-20 11:08:38,770 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid e3272006-5ef0-4545-8fb1-4cf4435a0d4d) service to CentOS.Hadoop/127.0.0.1:8020
2015-10-20 11:08:38,843 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 5 blocks total. Took 8 msec to generate and 63 msecs for RPC and NN processing.  Got back commands none
2015-10-20 11:08:38,850 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2015-10-20 11:08:38,854 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-10-20 11:08:38,863 INFO org.apache.hadoop.util.GSet: 0.5% max memory 966.7 MB = 4.8 MB
2015-10-20 11:08:38,863 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2015-10-20 11:08:38,864 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-340015217-127.0.0.1-1445079105718
2015-10-20 11:08:38,874 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-340015217-127.0.0.1-1445079105718 to blockPoolScannerMap, new size=1
2015-10-20 11:08:44,093 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-340015217-127.0.0.1-1445079105718:blk_1073741833_1009
2015-10-20 11:08:44,284 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-340015217-127.0.0.1-1445079105718:blk_1073741835_1011
2015-10-20 11:08:44,286 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-340015217-127.0.0.1-1445079105718:blk_1073741834_1010
2015-10-20 11:14:30,729 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-20 11:14:30,733 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
2015-10-20 11:14:36,613 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-20 11:14:37,565 WARN org.apache.hadoop.metrics2.impl.MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-datanode.properties,hadoop-metrics2.properties
2015-10-20 11:14:37,669 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-20 11:14:37,670 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-20 11:14:37,673 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: File descriptor passing is enabled.
2015-10-20 11:14:37,677 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is localhost.localdomain
2015-10-20 11:14:37,678 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-20 11:14:37,738 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-20 11:14:37,744 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-20 11:14:37,745 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Waiting for threadgroup to exit, active threads is 0
2015-10-20 11:14:37,745 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Shutdown complete.
2015-10-20 11:14:37,750 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Exception in secureMain
2015-10-20 11:14:37,758 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1
2015-10-20 11:14:37,761 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
2015-10-20 11:17:50,435 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-20 11:17:51,247 WARN org.apache.hadoop.metrics2.impl.MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-datanode.properties,hadoop-metrics2.properties
2015-10-20 11:17:51,346 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-20 11:17:51,346 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-20 11:17:51,350 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: File descriptor passing is enabled.
2015-10-20 11:17:51,351 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is localhost.localdomain
2015-10-20 11:17:51,351 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-20 11:17:51,402 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-20 11:17:51,404 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-20 11:17:51,407 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-20 11:17:51,407 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Listening on UNIX domain socket: /var/run/hadoop-hdfs/dn.50010
2015-10-20 11:17:51,553 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-20 11:17:51,573 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-20 11:17:51,581 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-20 11:17:51,582 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-20 11:17:51,582 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-20 11:17:51,583 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-20 11:17:51,611 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-10-20 11:17:51,621 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-10-20 11:17:51,621 INFO org.mortbay.log: jetty-6.1.26
2015-10-20 11:17:51,970 INFO org.mortbay.log: Started SelectChannelConnector@0.0.0.0:50075
2015-10-20 11:17:52,214 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-20 11:17:52,251 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-20 11:17:52,269 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-20 11:17:52,281 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-20 11:17:52,295 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-20 11:17:52,303 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to CentOS.Hadoop/127.0.0.1:8020 starting to offer service
2015-10-20 11:17:52,317 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-20 11:17:52,318 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-20 11:17:52,507 INFO org.apache.hadoop.hdfs.server.common.Storage: Data-node version: -55 and name-node layout version: -55
2015-10-20 11:17:52,512 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/in_use.lock acquired by nodename 7483@CentOS.Hadoop
2015-10-20 11:17:52,697 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-340015217-127.0.0.1-1445079105718
2015-10-20 11:17:52,697 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled
2015-10-20 11:17:52,698 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-10-20 11:17:52,699 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=113353421;bpid=BP-340015217-127.0.0.1-1445079105718;lv=-55;nsInfo=lv=-55;cid=CID-1ea80fc5-7f68-47f8-8615-fee51275fc9c;nsid=113353421;c=0;bpid=BP-340015217-127.0.0.1-1445079105718;dnuuid=e3272006-5ef0-4545-8fb1-4cf4435a0d4d
2015-10-20 11:17:52,729 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current, StorageType: DISK
2015-10-20 11:17:52,829 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-10-20 11:17:52,842 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1445327562842 with interval 21600000
2015-10-20 11:17:52,871 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-340015217-127.0.0.1-1445079105718
2015-10-20 11:17:52,872 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-340015217-127.0.0.1-1445079105718 on volume /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current...
2015-10-20 11:17:52,878 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current/BP-340015217-127.0.0.1-1445079105718/current: 2375680
2015-10-20 11:17:52,879 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-340015217-127.0.0.1-1445079105718 on /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current: 7ms
2015-10-20 11:17:52,879 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-340015217-127.0.0.1-1445079105718: 8ms
2015-10-20 11:17:52,879 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-340015217-127.0.0.1-1445079105718 on volume /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current...
2015-10-20 11:17:52,883 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-340015217-127.0.0.1-1445079105718 on volume /var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current: 4ms
2015-10-20 11:17:52,887 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 8ms
2015-10-20 11:17:52,888 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid null) service to CentOS.Hadoop/127.0.0.1:8020 beginning handshake with NN
2015-10-20 11:17:52,922 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid null) service to CentOS.Hadoop/127.0.0.1:8020 successfully registered with NN
2015-10-20 11:17:52,922 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode CentOS.Hadoop/127.0.0.1:8020 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-10-20 11:17:52,988 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid e3272006-5ef0-4545-8fb1-4cf4435a0d4d) service to CentOS.Hadoop/127.0.0.1:8020 trying to claim ACTIVE state with txid=196
2015-10-20 11:17:52,988 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-340015217-127.0.0.1-1445079105718 (Datanode Uuid e3272006-5ef0-4545-8fb1-4cf4435a0d4d) service to CentOS.Hadoop/127.0.0.1:8020
2015-10-20 11:17:53,033 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 5 blocks total. Took 1 msec to generate and 44 msecs for RPC and NN processing.  Got back commands none
2015-10-20 11:17:53,052 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2015-10-20 11:17:53,052 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-10-20 11:17:53,052 INFO org.apache.hadoop.util.GSet: 0.5% max memory 966.7 MB = 4.8 MB
2015-10-20 11:17:53,052 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2015-10-20 11:17:53,053 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-340015217-127.0.0.1-1445079105718
2015-10-20 11:17:53,056 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-340015217-127.0.0.1-1445079105718 to blockPoolScannerMap, new size=1
2015-10-20 11:18:48,169 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: cliID: DFSClient_NONMAPREDUCE_1343558526_1, src: 127.0.0.1, dest: 127.0.0.1, op: REQUEST_SHORT_CIRCUIT_SHM, shmId: 3c5889af6dca060827db5a0b010f6f74, srvID: e3272006-5ef0-4545-8fb1-4cf4435a0d4d, success: true
2015-10-20 11:18:48,334 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: 127.0.0.1, dest: 127.0.0.1, op: REQUEST_SHORT_CIRCUIT_FDS, blockid: 1073741833, srvID: e3272006-5ef0-4545-8fb1-4cf4435a0d4d, success: true
2015-10-20 11:21:46,109 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1564ms
2015-10-20 12:47:15,877 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-20 12:47:15,894 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 